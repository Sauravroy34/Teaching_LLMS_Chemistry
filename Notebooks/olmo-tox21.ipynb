{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n\n    # Optional: print the name of each GPU\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"CUDA is not available. No GPUs found.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:19:10.609823Z","iopub.execute_input":"2026-01-19T18:19:10.610182Z","iopub.status.idle":"2026-01-19T18:19:14.593754Z","shell.execute_reply.started":"2026-01-19T18:19:10.610151Z","shell.execute_reply":"2026-01-19T18:19:14.592906Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! pip install bitsandbytes\n! pip install peft\n! pip install --pre deepchem\n! pip install ai2-olmo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:19:35.959897Z","iopub.execute_input":"2026-01-19T18:19:35.960716Z","iopub.status.idle":"2026-01-19T18:20:06.941130Z","shell.execute_reply.started":"2026-01-19T18:19:35.960650Z","shell.execute_reply":"2026-01-19T18:20:06.940296Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.49.1\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\nCollecting deepchem\n  Downloading deepchem-2.8.1.dev20260114200409-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.5.3)\nCollecting numpy<2 (from deepchem)\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from deepchem) (2.2.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.6.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.13.3)\nRequirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.15.3)\nCollecting rdkit (from deepchem)\n  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2025.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit->deepchem) (11.3.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->deepchem) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->deepchem) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.17.0)\nDownloading deepchem-2.8.1.dev20260114200409-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, rdkit, deepchem\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepchem-2.8.1.dev20260114200409 numpy-1.26.4 rdkit-2025.9.3\nCollecting ai2-olmo\n  Downloading ai2_olmo-0.6.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (1.26.4)\nRequirement already satisfied: torch>=2.1 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.8.0+cu126)\nCollecting ai2-olmo-core==0.1.0 (from ai2-olmo)\n  Downloading ai2_olmo_core-0.1.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (14.2.0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (1.42.10)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.19.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (0.22.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (25.0)\nCollecting cached_path>=1.6.2 (from ai2-olmo)\n  Downloading cached_path-1.8.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (4.57.1)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (6.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (0.6.2)\nRequirement already satisfied: pydantic<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.12.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.32.5)\nCollecting rich (from ai2-olmo)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock<4.0,>=3.4 in /usr/local/lib/python3.12/dist-packages (from cached_path>=1.6.2->ai2-olmo) (3.20.1)\nRequirement already satisfied: huggingface-hub<2.0,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from cached_path>=1.6.2->ai2-olmo) (0.36.0)\nRequirement already satisfied: botocore<1.43.0,>=1.42.10 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (1.42.10)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (1.0.1)\nRequirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (0.16.0)\nRequirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.38.0)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.26.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (1.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ai2-olmo) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ai2-olmo) (2.19.2)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.4.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->ai2-olmo) (4.9.3)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->ai2-olmo) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->ai2-olmo) (2025.11.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->ai2-olmo) (4.67.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (2.6.2)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.71.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (5.29.5)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.26.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (4.9.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.8.1->cached_path>=1.6.2->ai2-olmo) (1.2.1rc0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ai2-olmo) (0.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.11)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (2025.11.12)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1->ai2-olmo) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1->ai2-olmo) (3.0.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.6.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (1.17.0)\nDownloading ai2_olmo-0.6.0-py3-none-any.whl (144.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.9/144.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ai2_olmo_core-0.1.0-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cached_path-1.8.1-py3-none-any.whl (37 kB)\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rich, cached_path, ai2-olmo-core, ai2-olmo\n  Attempting uninstall: rich\n    Found existing installation: rich 14.2.0\n    Uninstalling rich-14.2.0:\n      Successfully uninstalled rich-14.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ai2-olmo-0.6.0 ai2-olmo-core-0.1.0 cached_path-1.8.1 rich-13.9.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile train.py\nimport torch\nimport pytorch_lightning as pl\nimport deepchem as dc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom pytorch_lightning.callbacks import ModelCheckpoint \nfrom deepchem.molnet import load_tox21\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport seaborn as sns\nimport re\n\nclass OlmoDataset(Dataset):\n    def __init__(self, mode=\"Train\", max_length=350):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n\n        tasks, datasets, transformers = load_tox21(featurizer=\"raw\", splitter='scaffold')\n        train, valid, test = datasets\n        \n        self.task_names = tasks # List of 12 assay names (e.g. NR-AhR)\n\n        self.mode = mode.lower()\n        if self.mode == \"train\":\n            self.data = train\n        elif self.mode == \"valid\":\n            self.data = valid\n        elif self.mode == \"test\":\n            self.data = test\n\n        self.max_length = max_length\n        self.samples = []\n        self._filldataset()\n\n    def _filldataset(self):\n\n        for i in range(len(self.data)):\n            smiles = self.data.ids[i]\n            labels = self.data.y[i]   \n            weights = self.data.w[i]  #it has some missing data too\n\n            for task_idx, label in enumerate(labels):\n                #Only train on valid data (weight > 0)\n                if weights[task_idx] > 0:\n                    task_name = self.task_names[task_idx]\n                    self.samples.append(self._create_prompt(smiles, task_name, label))\n                    \n        print(f\"[{self.mode.upper()}] Number of samples: {len(self.samples)}\")\n\n    def _create_prompt(self, smiles, task_name, label):\n        eos_token = self.tokenizer.eos_token\n        \n        answer = \"Yes\" if label == 1.0 else \"No\"\n\n        full_prompt = (\n            \"### Instruction:\\n\"\n            f\"Is the following molecule toxic in the {task_name} assay?\\n\"\n            f\"{smiles}\\n\\n\"\n            \"### Response:\\n\"\n            f\"{answer}{eos_token}\"\n        )\n        return full_prompt\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        text = self.samples[idx]\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        input_ids = encodings[\"input_ids\"].squeeze(0)\n        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n        labels = input_ids.clone()\n\n        separator = \"### Response:\\n\"\n        parts = text.split(separator)\n\n        if len(parts) >= 2:\n            prompt_text = parts[0] + separator\n            prompt_encodings = self.tokenizer(\n                prompt_text,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            prompt_len = prompt_encodings[\"input_ids\"].shape[1]\n\n            if prompt_len < len(labels):\n                labels[:prompt_len] = -100\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\nclass OLMO_QLoRA(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.peft_config = LoraConfig(\n            r=32,\n            lora_alpha=64,\n            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM \n        )\n    def configure_model(self):\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            quantization_config=self.bnb_config,\n            trust_remote_code=True,\n        )\n        self.model = prepare_model_for_kbit_training(self.model)\n        self.model = get_peft_model(self.model, self.peft_config)\n        self.model.print_trainable_parameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        self.log(\"Train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        return loss\n\n    def on_train_end(self):\n            if self.trainer.is_global_zero:\n                print(\"\\nStarting test set evaluation (Accuracy) after training...\")\n\n                test_dataset = OlmoDataset(mode=\"test\", max_length=350)\n                test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n                self.model.eval()\n\n                preds = []\n                trues = []\n\n                print(f\"Evaluating on {len(test_loader)} samples...\")\n\n                with torch.no_grad():\n                    for i, batch in enumerate(test_loader):\n\n                        batch = {k: v.to(self.device) for k, v in batch.items()}\n\n                        input_ids = batch[\"input_ids\"]\n                        labels = batch[\"labels\"]\n                        attention_mask = batch[\"attention_mask\"]\n\n                        response_mask = (labels != -100)\n                        answer_start_index = response_mask.int().argmax(dim=1).item()\n\n                        if answer_start_index > 0:\n                            prompt_ids = input_ids[:, :answer_start_index]\n                            prompt_mask = attention_mask[:, :answer_start_index]\n                        else:\n                            prompt_ids = input_ids\n                            prompt_mask = attention_mask\n\n                        outputs = self.model.generate(\n                            input_ids=prompt_ids,\n                            attention_mask=prompt_mask,\n                            max_new_tokens=5, \n                            pad_token_id=self.tokenizer.eos_token_id,\n                            do_sample=False\n                        )\n\n                        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                        \n                        try:\n                            if \"### Response:\" in generated_text:\n                                response_part = generated_text.split(\"### Response:\")[-1].strip()\n                            else:\n                                response_part = generated_text.strip()\n\n                            # Simple string matching\n                            if \"Yes\" in response_part:\n                                pred_label = \"Yes\"\n                            elif \"No\" in response_part:\n                                pred_label = \"No\"\n                            else:\n                                pred_label = \"Invalid\"\n                            \n                            \n                            truth_ids = labels[0][labels[0] != -100]\n                            truth_text = self.tokenizer.decode(truth_ids, skip_special_tokens=True).strip()\n                            \n                            preds.append(pred_label)\n                            trues.append(truth_text)\n\n                        except Exception as e:\n                            print(f\"Error parsing prediction: {e}\")\n\n                        if i % 10 == 0:\n                            print(f\"Sample {i}: True={trues[-1]}, Pred={preds[-1]}\")\n\n                acc = accuracy_score(trues, preds)\n                \n                print(\"\\n=== Test Set Metrics ===\")\n                print(f\"Accuracy: {acc:.4f}\")\n                \n                roc = roc_auc_score(trues, preds)\n                print(f\"ROC: {roc:.4f}\")\n\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4,weight_decay=0.001)\n\n        total_steps = self.trainer.estimated_stepping_batches\n\n        warmup_steps = int(0.1*total_steps)\n        scheduler_warmup = LinearLR(\n            optimizer,\n            start_factor=0.01,\n            end_factor=1.0,\n            total_iters=warmup_steps,\n        )\n\n\n        scheduler_cosine = CosineAnnealingLR(\n            optimizer,\n            T_max=total_steps - warmup_steps,\n        )\n\n        scheduler = SequentialLR(\n            optimizer,\n            schedulers=[scheduler_warmup, scheduler_cosine],\n            milestones=[warmup_steps]\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\"\n            }\n        }\n\n\nif __name__ == \"__main__\":\n    dataset = OlmoDataset()\n    valid_dataset = OlmoDataset(mode=\"valid\")\n\n    train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n\n\n    \n    trainer = pl.Trainer(\n                accelerator=\"gpu\",\n                devices=2,\n                strategy=\"ddp\",\n                max_epochs=1, \n                precision=\"16-mixed\",\n                accumulate_grad_batches=8, \n                enable_checkpointing=False,\n                gradient_clip_val=0.5,\n                log_every_n_steps=10,\n                enable_progress_bar=False,\n            )\n    \n    model = OLMO_QLoRA()\n    \n    trainer.fit(model, train_loader, valid_loader)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:20:38.111912Z","iopub.execute_input":"2026-01-19T18:20:38.112222Z","iopub.status.idle":"2026-01-19T18:20:38.123510Z","shell.execute_reply.started":"2026-01-19T18:20:38.112196Z","shell.execute_reply":"2026-01-19T18:20:38.122645Z"}},"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:20:58.411165Z","iopub.execute_input":"2026-01-19T18:20:58.412095Z","iopub.status.idle":"2026-01-20T05:39:18.170765Z","shell.execute_reply.started":"2026-01-19T18:20:58.412063Z","shell.execute_reply":"2026-01-20T05:39:18.168713Z"}},"outputs":[{"name":"stdout","text":"No normalization for SPS. Feature removed!\nNo normalization for AvgIpc. Feature removed!\nNo normalization for NumAmideBonds. Feature removed!\nNo normalization for NumAtomStereoCenters. Feature removed!\nNo normalization for NumBridgeheadAtoms. Feature removed!\nNo normalization for NumHeterocycles. Feature removed!\nNo normalization for NumSpiroAtoms. Feature removed!\nNo normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\nNo normalization for Phi. Feature removed!\n2026-01-19 18:21:13.199115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768846873.396745     164 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768846873.455962     164 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768846873.922914     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846873.922957     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846873.922961     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846873.922964     164 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nWARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nexperimental_relax_shapes is deprecated, use reduce_retracing instead\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nSkipped loading some PyTorch models, missing a dependency. No module named 'dgl'\nNo module named 'dgl'\nSkipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\nSkipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\nSkipped loading some Jax models, missing a dependency. No module named 'haiku'\ntokenizer_config.json: 5.37kB [00:00, 21.5MB/s]\ntokenizer.json: 2.12MB [00:00, 39.8MB/s]\nspecial_tokens_map.json: 100%|████████████████| 65.0/65.0 [00:00<00:00, 605kB/s]\n[18:21:33] WARNING: not removing hydrogen atom without neighbors\n[18:21:33] Explicit valence for atom # 8 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 1322, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:33] Explicit valence for atom # 3 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 2290, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:33] Explicit valence for atom # 4 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 2297, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:33] Explicit valence for atom # 4 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 3558, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:34] Explicit valence for atom # 9 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 4565, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:34] Explicit valence for atom # 5 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 4649, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:34] Explicit valence for atom # 16 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 5538, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\n[18:21:34] Explicit valence for atom # 20 Al, 6, is greater than permitted\nWARNING:deepchem.feat.base_classes:Failed to featurize datapoint 6723, None. Appending empty array\nWARNING:deepchem.feat.base_classes:Exception message: Python argument types in\n    rdkit.Chem.rdmolfiles.CanonicalRankAtoms(NoneType)\ndid not match C++ signature:\n    CanonicalRankAtoms(RDKit::ROMol mol, bool breakTies=True, bool includeChirality=True, bool includeIsotopes=True, bool includeAtomMaps=True, bool includeChiralPresence=False)\nWARNING:deepchem.feat.base_classes:Exception message: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7831,) + inhomogeneous part.\n[18:21:35] WARNING: not removing hydrogen atom without neighbors\n[TRAIN] Number of samples: 63577\n[VALID] Number of samples: 7139\nUsing 16bit Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nNo normalization for SPS. Feature removed!\nNo normalization for AvgIpc. Feature removed!\nNo normalization for NumAmideBonds. Feature removed!\nNo normalization for NumAtomStereoCenters. Feature removed!\nNo normalization for NumBridgeheadAtoms. Feature removed!\nNo normalization for NumHeterocycles. Feature removed!\nNo normalization for NumSpiroAtoms. Feature removed!\nNo normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\nNo normalization for Phi. Feature removed!\n2026-01-19 18:22:14.333003: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768846934.356382     184 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768846934.363272     184 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768846934.381620     184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846934.381666     184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846934.381674     184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768846934.381682     184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nWARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nexperimental_relax_shapes is deprecated, use reduce_retracing instead\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nSkipped loading some PyTorch models, missing a dependency. No module named 'dgl'\nNo module named 'dgl'\nSkipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'dgl'\nSkipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\nSkipped loading some Jax models, missing a dependency. No module named 'haiku'\n[TRAIN] Number of samples: 63577\n[VALID] Number of samples: 7139\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nconfig.json: 100%|█████████████████████████████| 639/639 [00:00<00:00, 5.06MB/s]\nmodel.safetensors.index.json: 18.4kB [00:00, 50.5MB/s]\nFetching 6 files:   0%|                                   | 0/6 [00:00<?, ?it/s]\nmodel-00004-of-00006.safetensors:   0%|             | 0.00/4.86G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   0%|             | 0.00/4.94G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   0%|             | 0.00/2.98G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   0%|             | 0.00/4.86G [00:00<?, ?B/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   0%|    | 882k/4.99G [00:03<5:45:48, 241kB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   0%|    | 9.33M/4.86G [00:03<34:06, 2.37MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   0%|    | 20.8M/4.99G [00:04<13:21, 6.20MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:   1%|    | 30.8M/4.99G [00:04<08:29, 9.74MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:   1%|    | 47.8M/4.99G [00:04<04:55, 16.7MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   1%|    | 19.4M/2.98G [00:04<12:29, 3.95MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   0%|    | 20.0M/4.86G [00:05<18:28, 4.36MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   1%|    | 25.0M/4.94G [00:05<16:57, 4.83MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   1%|    | 36.0M/4.86G [00:05<09:19, 8.62MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   1%|    | 73.2M/4.99G [00:06<04:18, 19.0MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   2%|    | 47.3M/2.98G [00:06<05:30, 8.88MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   1%|    | 51.6M/4.86G [00:06<06:26, 12.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   2%|    | 87.8M/4.99G [00:06<03:19, 24.5MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   2%|    | 75.9M/4.94G [00:06<05:56, 13.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   0%|    | 15.4M/4.92G [00:06<35:04, 2.33MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   2%|     | 121M/4.99G [00:06<02:14, 36.1MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   2%|    | 88.5M/4.86G [00:06<03:34, 22.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   3%|▏    | 156M/4.99G [00:07<01:46, 45.5MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   2%|    | 69.0M/2.98G [00:07<04:27, 10.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   2%|    | 90.8M/4.94G [00:07<05:55, 13.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   3%|▏    | 141M/4.86G [00:07<02:18, 34.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   3%|▏    | 169M/4.99G [00:07<02:14, 35.7MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:   1%|    | 63.5M/4.92G [00:08<08:56, 9.06MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   4%|▏    | 193M/4.99G [00:08<02:08, 37.4MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   4%|▏    | 171M/4.86G [00:08<02:17, 34.2MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   5%|▏    | 225M/4.99G [00:08<01:33, 51.2MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:   6%|▎    | 277M/4.99G [00:08<00:56, 83.6MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   4%|▏    | 121M/2.98G [00:09<02:35, 18.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   4%|▏    | 218M/4.86G [00:09<01:35, 48.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   3%|▏    | 136M/4.94G [00:09<04:18, 18.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   2%|    | 94.3M/4.92G [00:09<06:02, 13.3MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   0%|   | 2.50M/4.86G [00:09<5:03:53, 266kB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   6%|▎    | 188M/2.98G [00:09<01:24, 33.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   3%|▏    | 140M/4.92G [00:09<03:37, 22.0MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:   9%|▍    | 255M/2.98G [00:10<00:58, 46.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:   5%|▎    | 265M/4.86G [00:10<01:48, 42.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   4%|▏    | 202M/4.94G [00:10<02:49, 27.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   6%|▎    | 312M/4.99G [00:10<01:48, 43.2MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:   8%|▍    | 391M/4.99G [00:10<01:01, 75.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   6%|▎    | 302M/4.86G [00:10<01:35, 47.9MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   4%|▏    | 187M/4.92G [00:11<02:58, 26.5MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:   9%|▍    | 434M/4.99G [00:11<00:53, 85.5MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   6%|▎    | 313M/4.86G [00:11<02:06, 36.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  10%|▌    | 511M/4.99G [00:12<00:49, 89.6MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   7%|▎    | 319M/4.86G [00:12<02:09, 35.1MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  11%|▌    | 324M/2.98G [00:12<01:04, 41.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   6%|▎    | 311M/4.94G [00:12<02:04, 37.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  12%|▌    | 578M/4.99G [00:12<00:47, 93.0MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:   9%|▍    | 453M/4.86G [00:12<00:52, 84.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  12%|▌    | 603M/4.99G [00:12<00:48, 90.7MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  12%|▌    | 624M/4.99G [00:14<01:18, 55.8MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  15%|▊    | 458M/2.98G [00:14<00:52, 47.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  10%|▍    | 485M/4.86G [00:14<01:36, 45.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  14%|▋    | 676M/4.99G [00:14<01:11, 60.7MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  11%|▌    | 552M/4.86G [00:15<01:05, 66.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:   9%|▍    | 426M/4.94G [00:15<01:51, 40.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   5%|▎    | 254M/4.92G [00:15<03:43, 20.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  15%|▋    | 743M/4.99G [00:15<00:57, 73.5MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  19%|▉    | 576M/2.98G [00:15<00:39, 61.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  16%|▊    | 810M/4.99G [00:16<00:48, 85.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  10%|▍    | 493M/4.94G [00:16<01:38, 45.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  13%|▋    | 619M/4.86G [00:16<01:13, 57.9MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   5%|▎    | 264M/4.92G [00:16<04:24, 17.6MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   1%|    | 50.7M/4.86G [00:17<22:53, 3.50MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  18%|▉    | 911M/4.99G [00:17<00:54, 74.7MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  21%|█    | 625M/2.98G [00:17<00:52, 45.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  12%|▌    | 595M/4.94G [00:19<01:46, 40.7MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   7%|▎    | 331M/4.92G [00:19<03:54, 19.6MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  14%|▋    | 686M/4.86G [00:19<01:57, 35.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  23%|█▏   | 676M/2.98G [00:22<01:28, 26.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:   8%|▍    | 388M/4.92G [00:23<04:26, 17.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   2%|    | 94.3M/4.86G [00:23<16:54, 4.69MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  25%|█▏   | 743M/2.98G [00:24<01:17, 29.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  19%|▉    | 968M/4.99G [00:25<03:07, 21.5MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:   8%|▍    | 414M/4.92G [00:26<05:13, 14.4MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   3%|▏    | 161M/4.86G [00:26<09:29, 8.24MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  21%|▊   | 1.03G/4.99G [00:27<02:41, 24.5MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  29%|█▍   | 877M/2.98G [00:27<01:04, 32.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  11%|▌    | 548M/4.92G [00:28<02:45, 26.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  13%|▋    | 627M/4.94G [00:29<01:45, 40.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   4%|▏    | 189M/4.86G [00:30<09:49, 7.92MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  31%|█▌   | 918M/2.98G [00:31<01:24, 24.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  14%|▋    | 695M/4.94G [00:32<04:26, 15.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  14%|▋    | 686M/4.86G [00:35<01:57, 35.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  32%|█▌   | 965M/2.98G [00:35<01:38, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  15%|▊    | 762M/4.94G [00:35<04:02, 17.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  12%|▌    | 615M/4.92G [00:36<04:27, 16.1MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  15%|▊    | 751M/4.86G [00:37<07:16, 9.41MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  33%|█▋   | 987M/2.98G [00:38<01:55, 17.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  18%|▉    | 882M/4.94G [00:38<02:58, 22.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   5%|▏    | 235M/4.86G [00:39<11:32, 6.68MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  13%|▋    | 625M/4.92G [00:40<05:33, 12.9MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  35%|█▍  | 1.05G/2.98G [00:40<01:33, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  22%|▊   | 1.09G/4.99G [00:40<05:57, 10.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  21%|▊   | 1.02G/4.94G [00:40<02:15, 28.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  13%|▋    | 642M/4.92G [00:41<05:36, 12.7MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  36%|█▍  | 1.07G/2.98G [00:41<01:37, 19.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  22%|▊   | 1.08G/4.94G [00:41<01:58, 32.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  23%|▉   | 1.15G/4.99G [00:45<05:47, 11.1MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  17%|▊    | 819M/4.86G [00:45<07:16, 9.25MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  37%|█▍  | 1.10G/2.98G [00:46<02:28, 12.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  23%|▉   | 1.14G/4.94G [00:46<02:30, 25.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   6%|▎    | 302M/4.86G [00:46<09:48, 7.74MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  38%|█▌  | 1.12G/2.98G [00:46<02:03, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  26%|█   | 1.28G/4.94G [00:46<01:33, 39.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  18%|▉    | 881M/4.86G [00:47<05:38, 11.7MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   7%|▎    | 319M/4.86G [00:47<09:20, 8.09MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  24%|▉   | 1.21G/4.99G [00:48<04:54, 12.9MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  14%|▋    | 709M/4.92G [00:48<06:05, 11.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  40%|█▌  | 1.19G/2.98G [00:48<01:23, 21.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  28%|█   | 1.36G/4.94G [00:48<01:28, 40.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  27%|█   | 1.34G/4.99G [00:48<02:36, 23.4MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  16%|▊    | 811M/4.92G [00:49<03:28, 19.8MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  41%|█▋  | 1.23G/2.98G [00:49<01:08, 25.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  28%|█▏  | 1.41G/4.94G [00:49<01:19, 44.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  20%|▉    | 948M/4.86G [00:49<04:26, 14.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  28%|█   | 1.38G/4.99G [00:49<02:18, 26.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  20%|▉    | 956M/4.86G [00:50<04:28, 14.5MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   7%|▎    | 338M/4.86G [00:51<10:04, 7.47MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  21%|▊   | 1.03G/4.86G [00:51<03:12, 19.8MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  18%|▉    | 878M/4.92G [00:51<03:08, 21.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  44%|█▋  | 1.30G/2.98G [00:54<01:25, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  30%|█▏  | 1.48G/4.94G [00:54<02:02, 28.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  24%|▉   | 1.15G/4.86G [00:54<02:22, 26.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  19%|▉    | 915M/4.92G [00:55<03:50, 17.4MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  47%|█▊  | 1.39G/2.98G [00:56<01:08, 23.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  28%|█▏  | 1.41G/4.99G [00:57<04:20, 13.7MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  31%|█▎  | 1.55G/4.94G [00:57<02:10, 26.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  48%|█▉  | 1.44G/2.98G [00:57<00:56, 27.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:   8%|▍    | 405M/4.86G [00:57<08:34, 8.66MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  25%|█   | 1.22G/4.86G [00:57<02:25, 25.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  11%|▌    | 538M/4.86G [00:58<03:43, 19.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  28%|█   | 1.34G/4.86G [00:58<01:29, 39.4MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  20%|▉    | 982M/4.92G [00:59<03:38, 18.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  32%|█▎  | 1.60G/4.94G [00:59<02:01, 27.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  12%|▌    | 606M/4.86G [00:59<02:48, 25.3MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  20%|█    | 995M/4.92G [00:59<03:21, 19.5MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  30%|█▏  | 1.45G/4.86G [00:59<01:04, 52.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  30%|█▏  | 1.47G/4.99G [00:59<03:28, 16.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  33%|█▎  | 1.62G/4.94G [00:59<01:55, 28.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  13%|▋    | 654M/4.86G [00:59<02:24, 29.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  31%|█▎  | 1.52G/4.86G [00:59<00:57, 58.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  33%|█▎  | 1.64G/4.94G [01:00<01:53, 29.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  14%|▋    | 673M/4.86G [01:00<02:27, 28.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  34%|█▎  | 1.65G/4.86G [01:00<00:42, 75.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  30%|█▏  | 1.49G/4.99G [01:01<03:45, 15.5MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  35%|█▍  | 1.75G/4.94G [01:01<01:13, 43.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  48%|█▉  | 1.44G/2.98G [01:01<01:40, 15.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  30%|█▏  | 1.50G/4.99G [01:02<03:50, 15.1MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  15%|▊    | 740M/4.86G [01:03<02:26, 28.2MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  22%|▊   | 1.07G/4.92G [01:03<03:37, 17.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  37%|█▍  | 1.81G/4.94G [01:03<01:28, 35.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  32%|█▎  | 1.60G/4.99G [01:04<02:24, 23.4MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  51%|██  | 1.51G/2.98G [01:06<01:36, 15.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  16%|▊    | 780M/4.86G [01:06<03:10, 21.5MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  34%|█▎  | 1.68G/4.99G [01:06<01:59, 27.7MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  23%|▉   | 1.13G/4.92G [01:07<03:38, 17.4MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  53%|██▏ | 1.60G/2.98G [01:09<01:11, 19.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  17%|▊    | 814M/4.86G [01:09<03:42, 18.2MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  37%|█▍  | 1.85G/4.94G [01:09<02:38, 19.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  36%|█▍  | 1.79G/4.99G [01:09<01:46, 30.0MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  54%|██▏ | 1.60G/2.98G [01:09<01:14, 18.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  37%|█▍  | 1.79G/4.86G [01:10<01:45, 29.2MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  24%|▉   | 1.20G/4.92G [01:10<03:16, 19.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  38%|█▌  | 1.90G/4.99G [01:10<01:12, 42.7MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  38%|█▌  | 1.89G/4.94G [01:10<02:20, 21.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  56%|██▏ | 1.67G/2.98G [01:10<00:49, 26.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  26%|█   | 1.27G/4.92G [01:11<02:20, 26.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  39%|█▌  | 1.97G/4.99G [01:11<01:01, 49.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  38%|█▌  | 1.86G/4.86G [01:11<01:34, 31.7MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  28%|█   | 1.38G/4.92G [01:12<01:29, 39.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  41%|█▌  | 2.03G/4.99G [01:12<01:02, 47.1MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  39%|█▌  | 1.94G/4.94G [01:12<02:13, 22.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  58%|██▎ | 1.74G/2.98G [01:12<00:45, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  18%|▉    | 883M/4.86G [01:13<03:52, 17.1MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  42%|█▋  | 2.09G/4.99G [01:13<00:57, 50.8MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  29%|█▏  | 1.45G/4.92G [01:13<01:26, 40.0MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  60%|██▍ | 1.80G/2.98G [01:14<00:36, 32.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  41%|█▋  | 2.01G/4.94G [01:14<01:47, 27.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  45%|█▊  | 2.23G/4.99G [01:17<01:09, 39.8MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  20%|▉    | 955M/4.86G [01:18<04:03, 16.0MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  42%|█▋  | 2.08G/4.94G [01:19<02:20, 20.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  41%|█▋  | 1.98G/4.86G [01:20<02:08, 22.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  46%|█▊  | 2.31G/4.99G [01:21<01:17, 34.5MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  62%|██▍ | 1.86G/2.98G [01:23<01:15, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  69%|██▊ | 2.06G/2.98G [01:26<00:35, 26.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  21%|▊   | 1.02G/4.86G [01:27<05:20, 12.0MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  44%|█▊  | 2.17G/4.94G [01:27<02:59, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  71%|██▊ | 2.12G/2.98G [01:27<00:28, 30.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  21%|▊   | 1.03G/4.86G [01:27<05:27, 11.7MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  44%|█▊  | 2.18G/4.94G [01:27<03:00, 15.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  74%|██▉ | 2.22G/2.98G [01:28<00:19, 39.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  31%|█▏  | 1.52G/4.92G [01:29<04:33, 12.4MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  76%|███ | 2.26G/2.98G [01:29<00:18, 39.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  23%|▉   | 1.11G/4.86G [01:30<03:41, 16.9MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  46%|█▊  | 2.31G/4.99G [01:30<03:16, 13.6MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  41%|█▋  | 1.99G/4.86G [01:30<03:56, 12.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  45%|█▊  | 2.20G/4.94G [01:31<03:40, 12.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  77%|███ | 2.29G/2.98G [01:32<00:23, 29.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  42%|█▋  | 2.03G/4.86G [01:32<03:30, 13.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  46%|█▊  | 2.32G/4.99G [01:32<03:38, 12.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  43%|█▋  | 2.08G/4.86G [01:32<02:39, 17.4MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  32%|█▎  | 1.59G/4.92G [01:34<04:32, 12.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  46%|█▊  | 2.30G/4.94G [01:35<02:42, 16.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  48%|█▉  | 2.39G/4.99G [01:37<03:26, 12.6MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  24%|▉   | 1.17G/4.86G [01:37<05:02, 12.2MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  33%|█▎  | 1.62G/4.92G [01:37<04:35, 12.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  27%|█   | 1.29G/4.86G [01:38<02:42, 22.0MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  43%|█▋  | 2.11G/4.86G [01:38<03:50, 11.9MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  34%|█▎  | 1.69G/4.92G [01:38<03:12, 16.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  48%|█▉  | 2.36G/4.94G [01:38<02:26, 17.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  45%|█▊  | 2.16G/4.86G [01:39<02:43, 16.4MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  80%|███▏| 2.38G/2.98G [01:39<00:32, 18.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  27%|█   | 1.33G/4.86G [01:39<02:27, 23.9MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  45%|█▊  | 2.21G/4.86G [01:39<02:02, 21.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  82%|███▎| 2.44G/2.98G [01:39<00:21, 25.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  36%|█▍  | 1.76G/4.92G [01:39<02:27, 21.5MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  28%|█   | 1.34G/4.86G [01:39<02:21, 24.7MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  49%|█▉  | 2.45G/4.99G [01:40<02:53, 14.7MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  46%|█▊  | 2.25G/4.86G [01:40<01:42, 25.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  83%|███▎| 2.48G/2.98G [01:40<00:17, 28.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  37%|█▍  | 1.82G/4.92G [01:40<01:51, 27.8MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  38%|█▌  | 1.85G/4.92G [01:41<01:46, 28.8MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  48%|█▉  | 2.32G/4.86G [01:41<01:20, 31.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  49%|█▉  | 2.40G/4.94G [01:41<02:31, 16.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  84%|███▎| 2.50G/2.98G [01:42<00:20, 23.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  29%|█▏  | 1.39G/4.86G [01:42<02:34, 22.4MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  50%|█▉  | 2.46G/4.94G [01:43<02:03, 20.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  30%|█▏  | 1.45G/4.86G [01:43<01:53, 29.9MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  49%|█▉  | 2.40G/4.86G [01:44<01:20, 30.6MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  49%|█▉  | 2.47G/4.99G [01:46<04:08, 10.2MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  31%|█▏  | 1.52G/4.86G [01:47<02:33, 21.8MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  86%|███▍| 2.57G/2.98G [01:48<00:23, 17.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  39%|█▌  | 1.92G/4.92G [01:48<02:48, 17.8MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  50%|██  | 2.45G/4.86G [01:48<01:45, 22.9MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  91%|███▋| 2.71G/2.98G [01:48<00:07, 35.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  51%|██  | 2.54G/4.99G [01:48<03:05, 13.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  52%|██  | 2.52G/4.86G [01:49<01:22, 28.5MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  93%|███▋| 2.77G/2.98G [01:49<00:05, 38.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  32%|█▎  | 1.57G/4.86G [01:49<02:20, 23.4MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  52%|██  | 2.59G/4.99G [01:49<02:13, 17.9MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  40%|█▌  | 1.98G/4.92G [01:49<02:16, 21.6MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  94%|███▊| 2.80G/2.98G [01:49<00:04, 41.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  34%|█▎  | 1.64G/4.86G [01:49<01:37, 33.1MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  54%|██▏ | 2.63G/4.86G [01:50<00:51, 43.1MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  53%|██▏ | 2.66G/4.99G [01:50<01:33, 24.9MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  40%|█▌  | 1.99G/4.92G [01:50<02:25, 20.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  51%|██  | 2.53G/4.94G [01:50<02:48, 14.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  54%|██▏ | 2.70G/4.99G [01:50<01:14, 30.5MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  55%|██▏ | 2.67G/4.86G [01:50<00:47, 46.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  53%|██  | 2.62G/4.94G [01:51<01:51, 20.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  55%|██▏ | 2.74G/4.99G [01:51<01:11, 31.3MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  35%|█▍  | 1.70G/4.86G [01:56<02:51, 18.4MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  42%|█▋  | 2.05G/4.92G [01:56<03:23, 14.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  54%|██▏ | 2.69G/4.94G [01:56<02:06, 17.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  43%|█▋  | 2.14G/4.92G [01:57<02:04, 22.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  56%|██▏ | 2.75G/4.94G [01:57<01:33, 23.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  58%|██▎ | 2.88G/4.99G [01:58<01:22, 25.5MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  95%|███▊| 2.82G/2.98G [01:58<00:11, 14.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  57%|██▎ | 2.84G/4.94G [01:58<01:07, 31.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  59%|██▎ | 2.96G/4.99G [01:58<00:58, 34.5MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  58%|██▎ | 2.87G/4.94G [01:58<00:57, 35.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  36%|█▍  | 1.77G/4.86G [01:59<02:27, 20.9MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  96%|███▊| 2.87G/2.98G [01:59<00:07, 16.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  44%|█▊  | 2.16G/4.92G [01:59<02:18, 20.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  38%|█▌  | 1.87G/4.86G [01:59<01:31, 32.6MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  62%|██▍ | 3.09G/4.99G [01:59<00:37, 50.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  59%|██▎ | 2.92G/4.94G [02:00<00:55, 36.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  98%|███▉| 2.92G/2.98G [02:00<00:03, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  41%|█▋  | 1.99G/4.86G [02:00<00:59, 47.9MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  64%|██▌ | 3.20G/4.99G [02:00<00:29, 61.8MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  56%|██▏ | 2.71G/4.86G [02:01<02:33, 14.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  42%|█▋  | 2.05G/4.86G [02:01<00:57, 48.7MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  57%|██▎ | 2.77G/4.86G [02:02<01:56, 18.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  60%|██▍ | 2.98G/4.94G [02:02<01:04, 30.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  44%|█▊  | 2.16G/4.92G [02:03<03:33, 12.9MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  63%|██▌ | 3.10G/4.94G [02:03<00:33, 54.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  67%|██▋ | 3.33G/4.99G [02:03<00:32, 51.4MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  64%|██▌ | 3.16G/4.94G [02:04<00:37, 47.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  44%|█▊  | 2.13G/4.86G [02:05<01:22, 33.1MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  46%|█▊  | 2.28G/4.92G [02:06<02:11, 20.2MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  44%|█▊  | 2.14G/4.86G [02:06<01:27, 31.2MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  68%|██▋ | 3.40G/4.99G [02:06<00:38, 41.7MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  65%|██▌ | 3.23G/4.94G [02:07<00:42, 40.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  47%|█▊  | 2.30G/4.92G [02:07<02:17, 19.2MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  59%|██▍ | 2.89G/4.86G [02:08<01:41, 19.5MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  45%|█▊  | 2.17G/4.86G [02:09<01:57, 22.8MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  70%|██▊ | 3.48G/4.99G [02:10<00:43, 34.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  67%|██▋ | 3.29G/4.94G [02:10<00:51, 31.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  47%|█▉  | 2.28G/4.86G [02:10<01:14, 34.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  67%|██▋ | 3.33G/4.94G [02:11<00:47, 33.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  48%|█▉  | 2.37G/4.92G [02:11<02:21, 18.1MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  48%|█▉  | 2.39G/4.92G [02:13<02:25, 17.4MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  48%|█▉  | 2.33G/4.86G [02:13<01:27, 29.0MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  49%|█▉  | 2.38G/4.86G [02:14<01:11, 34.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  68%|██▋ | 3.35G/4.94G [02:16<01:33, 17.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  50%|█▉  | 2.45G/4.92G [02:17<02:27, 16.8MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors:  98%|███▉| 2.92G/2.98G [02:19<00:03, 21.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  69%|██▊ | 3.40G/4.94G [02:21<01:50, 13.9MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  51%|██  | 2.52G/4.92G [02:21<02:24, 16.6MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  50%|█▉  | 2.43G/4.86G [02:21<02:32, 16.0MB/s]\u001b[A\n\n\n\nmodel-00006-of-00006.safetensors: 100%|████| 2.98G/2.98G [02:21<00:00, 6.71MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n\nmodel-00006-of-00006.safetensors: 100%|████| 2.98G/2.98G [02:21<00:00, 21.0MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00006.safetensors:  71%|██▊ | 3.53G/4.99G [02:22<01:42, 14.3MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  53%|██  | 2.59G/4.92G [02:22<01:45, 22.2MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  71%|██▊ | 3.53G/4.99G [02:22<01:44, 13.9MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  51%|██  | 2.46G/4.86G [02:23<02:26, 16.4MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  71%|██▊ | 3.52G/4.94G [02:23<01:04, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  60%|██▍ | 2.94G/4.86G [02:23<03:27, 9.23MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  72%|██▉ | 3.59G/4.99G [02:23<01:15, 18.5MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  53%|██  | 2.57G/4.86G [02:23<01:13, 31.1MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  71%|██▊ | 3.53G/4.94G [02:23<01:02, 22.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  61%|██▍ | 2.96G/4.86G [02:24<03:04, 10.3MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  54%|██▏ | 2.62G/4.86G [02:24<01:00, 37.1MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  72%|██▊ | 3.54G/4.94G [02:24<00:59, 23.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  55%|██▏ | 2.66G/4.86G [02:24<00:50, 43.7MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  62%|██▍ | 3.01G/4.86G [02:24<02:17, 13.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  72%|██▉ | 3.58G/4.94G [02:24<00:47, 28.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  53%|██▏ | 2.63G/4.92G [02:24<01:50, 20.7MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  64%|██▌ | 3.09G/4.86G [02:25<01:28, 19.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  73%|██▉ | 3.63G/4.99G [02:26<01:14, 18.1MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  64%|██▌ | 3.11G/4.86G [02:27<01:31, 19.2MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  55%|██▏ | 2.70G/4.92G [02:28<01:51, 20.0MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  66%|██▋ | 3.21G/4.86G [02:28<00:57, 28.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  74%|██▉ | 3.70G/4.99G [02:29<01:11, 18.2MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  55%|██▏ | 2.68G/4.86G [02:30<02:09, 16.8MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  67%|██▋ | 3.28G/4.86G [02:30<00:53, 29.5MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  75%|███ | 3.77G/4.99G [02:32<01:00, 20.2MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  76%|███ | 3.79G/4.99G [02:33<00:56, 21.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  68%|██▋ | 3.32G/4.86G [02:33<01:01, 24.9MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  77%|███ | 3.86G/4.99G [02:33<00:34, 32.4MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  78%|███▏| 3.92G/4.99G [02:34<00:26, 40.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  69%|██▋ | 3.34G/4.86G [02:34<01:05, 23.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  79%|███▏| 3.93G/4.99G [02:34<00:28, 37.4MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  71%|██▊ | 3.45G/4.86G [02:35<00:34, 40.5MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  72%|██▊ | 3.48G/4.86G [02:36<00:35, 38.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  73%|██▉ | 3.62G/4.94G [02:36<02:33, 8.63MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  80%|███▏| 3.97G/4.99G [02:37<00:38, 26.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  73%|██▉ | 3.53G/4.86G [02:39<00:47, 28.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  56%|██▏ | 2.76G/4.92G [02:39<01:48, 20.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  81%|███▏| 4.03G/4.99G [02:40<00:37, 25.4MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  73%|██▉ | 3.55G/4.86G [02:40<00:50, 25.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  82%|███▎| 4.09G/4.99G [02:41<00:29, 30.3MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  57%|██▎ | 2.83G/4.92G [02:42<02:44, 12.8MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  74%|██▉ | 3.61G/4.86G [02:42<00:47, 26.4MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  75%|███ | 3.64G/4.86G [02:43<00:41, 29.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  75%|██▉ | 3.68G/4.94G [02:44<02:19, 8.97MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  57%|██▎ | 2.75G/4.86G [02:44<04:02, 8.68MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  83%|███▎| 4.15G/4.99G [02:44<00:34, 24.5MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  76%|███ | 3.71G/4.86G [02:44<00:30, 37.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  75%|███ | 3.72G/4.94G [02:44<01:46, 11.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  84%|███▎| 4.20G/4.99G [02:44<00:24, 31.9MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  58%|██▎ | 2.81G/4.86G [02:44<02:41, 12.6MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  85%|███▍| 4.23G/4.99G [02:44<00:20, 37.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  78%|███ | 3.84G/4.94G [02:45<00:47, 23.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00004-of-00006.safetensors:  59%|██▎ | 2.85G/4.86G [02:45<02:08, 15.6MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  78%|███▏| 3.80G/4.86G [02:45<00:21, 50.1MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  60%|██▍ | 2.96G/4.92G [02:45<01:52, 17.4MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  61%|██▍ | 2.98G/4.86G [02:45<01:00, 31.2MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  85%|███▍| 4.26G/4.99G [02:45<00:20, 35.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  79%|███▏| 3.89G/4.94G [02:46<00:38, 26.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  63%|██▌ | 3.05G/4.86G [02:46<00:45, 39.3MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  80%|███▏| 3.95G/4.94G [02:46<00:28, 34.4MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  87%|███▍| 4.32G/4.99G [02:46<00:13, 49.1MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  80%|███▏| 3.86G/4.86G [02:47<00:20, 48.0MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  82%|███▎| 4.06G/4.94G [02:47<00:17, 50.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  88%|███▌| 4.38G/4.99G [02:47<00:12, 50.3MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  61%|██▍ | 3.01G/4.92G [02:48<01:44, 18.3MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  81%|███▏| 3.93G/4.86G [02:48<00:18, 49.2MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  64%|██▌ | 3.14G/4.92G [02:49<01:07, 26.6MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  66%|██▋ | 3.25G/4.92G [02:51<00:51, 32.4MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  64%|██▌ | 3.13G/4.86G [02:53<01:22, 20.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  85%|███▍| 4.18G/4.94G [02:55<00:28, 26.2MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  69%|██▋ | 3.38G/4.92G [02:55<00:47, 32.2MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  82%|███▎| 3.96G/4.86G [02:56<00:57, 15.6MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  88%|███▌| 4.40G/4.99G [02:56<00:45, 12.8MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  89%|███▌| 4.46G/4.99G [02:56<00:27, 19.2MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  82%|███▎| 3.97G/4.86G [02:56<00:54, 16.1MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  87%|███▍| 4.31G/4.94G [02:57<00:17, 35.4MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  82%|███▎| 3.99G/4.86G [02:56<00:47, 18.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  90%|███▌| 4.48G/4.99G [02:57<00:22, 22.4MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  91%|███▋| 4.55G/4.99G [02:57<00:14, 31.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  82%|███▎| 4.01G/4.86G [02:57<00:45, 18.6MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  92%|███▋| 4.58G/4.99G [02:58<00:11, 36.9MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  83%|███▎| 4.04G/4.86G [02:58<00:32, 25.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  89%|███▌| 4.41G/4.94G [02:58<00:12, 40.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  93%|███▋| 4.67G/4.99G [02:58<00:05, 56.9MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  70%|██▊ | 3.45G/4.92G [02:58<00:49, 29.6MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  85%|███▍| 4.12G/4.86G [02:59<00:17, 42.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  90%|███▌| 4.43G/4.94G [02:59<00:13, 38.8MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  72%|██▉ | 3.55G/4.92G [02:59<00:33, 41.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  95%|███▊| 4.73G/4.99G [02:59<00:04, 60.3MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  65%|██▌ | 3.15G/4.86G [03:00<02:10, 13.1MB/s]\u001b[A\nmodel-00002-of-00006.safetensors:  97%|███▊| 4.83G/4.99G [03:00<00:02, 79.6MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  91%|███▋| 4.48G/4.94G [03:01<00:12, 37.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  86%|███▍| 4.19G/4.86G [03:01<00:17, 38.0MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  88%|███▌| 4.26G/4.86G [03:02<00:14, 41.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  75%|██▉ | 3.69G/4.92G [03:03<00:32, 37.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  98%|███▉| 4.88G/4.99G [03:04<00:02, 38.4MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  92%|███▋| 4.55G/4.94G [03:04<00:14, 27.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  67%|██▋ | 3.25G/4.86G [03:04<01:38, 16.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  89%|███▌| 4.31G/4.86G [03:05<00:17, 31.5MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  76%|███ | 3.74G/4.92G [03:06<00:36, 32.4MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors:  99%|███▉| 4.94G/4.99G [03:06<00:01, 33.4MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  68%|██▋ | 3.30G/4.86G [03:06<01:30, 17.3MB/s]\u001b[A\n\nmodel-00005-of-00006.safetensors:  90%|███▌| 4.38G/4.86G [03:07<00:14, 32.6MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  91%|███▋| 4.41G/4.86G [03:07<00:11, 39.0MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  92%|███▋| 4.46G/4.86G [03:07<00:07, 52.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00006.safetensors: 100%|████| 4.99G/4.99G [03:07<00:00, 26.6MB/s]\u001b[A\n\n\nmodel-00005-of-00006.safetensors:  93%|███▋| 4.53G/4.86G [03:07<00:04, 68.3MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  95%|███▊| 4.60G/4.86G [03:08<00:03, 80.8MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  95%|███▊| 4.63G/4.86G [03:08<00:02, 84.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  93%|███▋| 4.58G/4.94G [03:09<00:19, 18.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  97%|███▊| 4.70G/4.86G [03:09<00:01, 88.5MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  97%|███▉| 4.72G/4.86G [03:09<00:01, 86.7MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  98%|███▉| 4.77G/4.86G [03:10<00:00, 83.5MB/s]\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors:  99%|███▉| 4.82G/4.86G [03:11<00:00, 84.3MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  93%|███▋| 4.61G/4.94G [03:11<00:18, 18.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00005-of-00006.safetensors: 100%|████| 4.86G/4.86G [03:11<00:00, 25.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00003-of-00006.safetensors:  77%|███ | 3.80G/4.92G [03:11<00:51, 22.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  68%|██▋ | 3.32G/4.86G [03:12<02:02, 12.5MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  78%|███▏| 3.86G/4.92G [03:12<00:38, 27.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  94%|███▋| 4.62G/4.94G [03:12<00:19, 16.1MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  69%|██▊ | 3.37G/4.86G [03:13<01:35, 15.6MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  79%|███▏| 3.89G/4.92G [03:13<00:37, 27.9MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  70%|██▊ | 3.42G/4.86G [03:13<01:05, 21.8MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  94%|███▊| 4.63G/4.94G [03:13<00:18, 16.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  95%|███▊| 4.70G/4.94G [03:14<00:09, 26.3MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  71%|██▊ | 3.44G/4.86G [03:15<01:17, 18.3MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  72%|██▉ | 3.51G/4.86G [03:16<00:53, 25.4MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  80%|███▏| 3.95G/4.92G [03:16<00:40, 24.2MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  73%|██▉ | 3.53G/4.86G [03:17<00:49, 26.6MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  97%|███▊| 4.77G/4.94G [03:17<00:06, 24.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  75%|██▉ | 3.62G/4.86G [03:17<00:26, 45.9MB/s]\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  98%|███▉| 4.84G/4.94G [03:17<00:02, 37.5MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  81%|███▏| 3.98G/4.92G [03:17<00:38, 24.7MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  76%|███ | 3.69G/4.86G [03:18<00:19, 60.5MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  82%|███▎| 4.05G/4.92G [03:18<00:25, 34.8MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors:  99%|███▉| 4.91G/4.94G [03:18<00:00, 47.3MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  85%|███▍| 4.17G/4.92G [03:18<00:11, 64.4MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00006.safetensors: 100%|████| 4.94G/4.94G [03:18<00:00, 24.9MB/s]\u001b[A\u001b[A\u001b[A\nFetching 6 files:  17%|████▎                     | 1/6 [03:19<16:35, 199.06s/it]\n\nmodel-00003-of-00006.safetensors:  86%|███▍| 4.25G/4.92G [03:18<00:07, 86.2MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  77%|███ | 3.74G/4.86G [03:18<00:17, 62.8MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  87%|███▍| 4.30G/4.92G [03:18<00:06, 98.4MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  79%|███▏| 3.86G/4.86G [03:19<00:10, 91.7MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  81%|████ | 3.92G/4.86G [03:19<00:08, 111MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  81%|████ | 3.95G/4.86G [03:19<00:07, 119MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  82%|████ | 4.00G/4.86G [03:20<00:06, 139MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  89%|███▌| 4.38G/4.92G [03:20<00:06, 79.6MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  83%|████▏| 4.03G/4.86G [03:20<00:06, 121MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  84%|███▎| 4.09G/4.86G [03:21<00:08, 94.5MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  91%|███▌| 4.46G/4.92G [03:21<00:06, 68.7MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  91%|███▋| 4.48G/4.92G [03:21<00:06, 71.6MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00006.safetensors:  91%|███▋| 4.50G/4.92G [03:22<00:06, 69.5MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  85%|███▍| 4.11G/4.86G [03:26<00:35, 21.2MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  92%|███▋| 4.51G/4.92G [03:26<00:20, 20.0MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  85%|███▍| 4.13G/4.86G [03:27<00:30, 23.5MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  86%|███▍| 4.16G/4.86G [03:27<00:22, 31.4MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  87%|███▍| 4.21G/4.86G [03:28<00:16, 38.6MB/s]\u001b[A\nmodel-00004-of-00006.safetensors:  87%|███▍| 4.24G/4.86G [03:28<00:12, 49.1MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  92%|███▋| 4.55G/4.92G [03:28<00:17, 21.8MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  90%|███▌| 4.35G/4.86G [03:28<00:05, 99.9MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  94%|███▋| 4.61G/4.92G [03:28<00:09, 34.6MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  91%|████▌| 4.44G/4.86G [03:28<00:03, 121MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  95%|███▊| 4.66G/4.92G [03:28<00:06, 44.2MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  95%|████▋| 4.61G/4.86G [03:29<00:01, 212MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  97%|███▉| 4.79G/4.92G [03:29<00:01, 93.6MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors:  96%|████▊| 4.68G/4.86G [03:29<00:00, 249MB/s]\u001b[A\n\nmodel-00003-of-00006.safetensors:  99%|████▉| 4.86G/4.92G [03:29<00:00, 114MB/s]\u001b[A\u001b[A\nmodel-00004-of-00006.safetensors: 100%|████| 4.86G/4.86G [03:29<00:00, 23.2MB/s]\u001b[A\n\n\nmodel-00003-of-00006.safetensors: 100%|████| 4.92G/4.92G [03:29<00:00, 23.5MB/s]\u001b[A\u001b[A\nFetching 6 files: 100%|███████████████████████████| 6/6 [03:30<00:00, 35.03s/it]\nFetching 6 files: 100%|███████████████████████████| 6/6 [03:30<00:00, 35.03s/it]\nLoading checkpoint shards: 100%|██████████████████| 6/6 [02:36<00:00, 26.00s/it]\nLoading checkpoint shards: 100%|██████████████████| 6/6 [02:36<00:00, 26.03s/it]\ngeneration_config.json: 100%|███████████████████| 116/116 [00:00<00:00, 846kB/s]\ntrainable params: 79,953,920 || all params: 6,968,049,664 || trainable%: 1.1474\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\ntrainable params: 79,953,920 || all params: 6,968,049,664 || trainable%: 1.1474\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\nLoading `train_dataloader` to estimate number of stepping batches.\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ PeftModelForCausalLM │  3.7 B │ train │     0 │\n└───┴───────┴──────────────────────┴────────┴───────┴───────┘\n\u001b[1mTrainable params\u001b[0m: 80.0 M                                                        \n\u001b[1mNon-trainable params\u001b[0m: 3.7 B                                                     \n\u001b[1mTotal params\u001b[0m: 3.7 B                                                             \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 14.9 K                                  \n\u001b[1mModules in train mode\u001b[0m: 2242                                                     \n\u001b[1mModules in eval mode\u001b[0m: 423                                                       \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 423 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: It is recommended to use `self.log('Train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\nStarting test set evaluation (Accuracy) after training...\n[TEST] Number of samples: 7148\nEvaluating on 7148 samples...\nSample 0: True=No, Pred=No\nSample 10: True=No, Pred=No\nSample 20: True=No, Pred=No\nSample 30: True=No, Pred=No\nSample 40: True=No, Pred=No\nSample 50: True=No, Pred=No\nSample 60: True=No, Pred=No\nSample 70: True=No, Pred=No\nSample 80: True=No, Pred=No\nSample 90: True=No, Pred=No\nSample 100: True=No, Pred=No\nSample 110: True=No, Pred=No\nSample 120: True=No, Pred=No\nSample 130: True=No, Pred=No\nSample 140: True=No, Pred=No\nSample 150: True=No, Pred=No\nSample 160: True=No, Pred=No\nSample 170: True=No, Pred=No\nSample 180: True=Yes, Pred=No\nSample 190: True=No, Pred=No\nSample 200: True=Yes, Pred=No\nSample 210: True=No, Pred=No\nSample 220: True=Yes, Pred=No\nSample 230: True=No, Pred=No\nSample 240: True=No, Pred=No\nSample 250: True=No, Pred=No\nSample 260: True=No, Pred=No\nSample 270: True=No, Pred=No\nSample 280: True=No, Pred=No\nSample 290: True=No, Pred=No\nSample 300: True=No, Pred=No\nSample 310: True=No, Pred=No\nSample 320: True=No, Pred=No\nSample 330: True=No, Pred=No\nSample 340: True=No, Pred=No\nSample 350: True=No, Pred=No\nSample 360: True=No, Pred=No\nSample 370: True=No, Pred=No\nSample 380: True=No, Pred=No\nSample 390: True=No, Pred=No\nSample 400: True=No, Pred=No\nSample 410: True=No, Pred=No\nSample 420: True=No, Pred=No\nSample 430: True=No, Pred=No\nSample 440: True=No, Pred=No\nSample 450: True=No, Pred=No\nSample 460: True=No, Pred=No\nSample 470: True=No, Pred=No\nSample 480: True=No, Pred=No\nSample 490: True=No, Pred=No\nSample 500: True=No, Pred=No\nSample 510: True=No, Pred=No\nSample 520: True=No, Pred=No\nSample 530: True=No, Pred=No\nSample 540: True=No, Pred=No\nSample 550: True=Yes, Pred=No\nSample 560: True=Yes, Pred=No\nSample 570: True=No, Pred=No\nSample 580: True=No, Pred=No\nSample 590: True=No, Pred=No\nSample 600: True=No, Pred=No\nSample 610: True=No, Pred=No\nSample 620: True=No, Pred=No\nSample 630: True=No, Pred=No\nSample 640: True=No, Pred=No\nSample 650: True=Yes, Pred=No\nSample 660: True=No, Pred=No\nSample 670: True=No, Pred=No\nSample 680: True=No, Pred=No\nSample 690: True=No, Pred=No\nSample 700: True=No, Pred=No\nSample 710: True=No, Pred=No\nSample 720: True=Yes, Pred=No\nSample 730: True=No, Pred=No\nSample 740: True=No, Pred=No\nSample 750: True=No, Pred=No\nSample 760: True=No, Pred=No\nSample 770: True=No, Pred=No\nSample 780: True=Yes, Pred=No\nSample 790: True=No, Pred=No\nSample 800: True=No, Pred=No\nSample 810: True=No, Pred=No\nSample 820: True=No, Pred=No\nSample 830: True=No, Pred=No\nSample 840: True=No, Pred=No\nSample 850: True=Yes, Pred=No\nSample 860: True=No, Pred=No\nSample 870: True=Yes, Pred=No\nSample 880: True=No, Pred=No\nSample 890: True=No, Pred=No\nSample 900: True=No, Pred=No\nSample 910: True=No, Pred=No\nSample 920: True=No, Pred=No\nSample 930: True=No, Pred=No\nSample 940: True=No, Pred=No\nSample 950: True=No, Pred=No\nSample 960: True=No, Pred=No\nSample 970: True=No, Pred=No\nSample 980: True=No, Pred=No\nSample 990: True=No, Pred=No\nSample 1000: True=No, Pred=No\nSample 1010: True=No, Pred=No\nSample 1020: True=No, Pred=No\nSample 1030: True=No, Pred=No\nSample 1040: True=No, Pred=No\nSample 1050: True=Yes, Pred=No\nSample 1060: True=No, Pred=No\nSample 1070: True=No, Pred=No\nSample 1080: True=No, Pred=No\nSample 1090: True=No, Pred=No\nSample 1100: True=No, Pred=No\nSample 1110: True=No, Pred=No\nSample 1120: True=No, Pred=No\nSample 1130: True=No, Pred=No\nSample 1140: True=Yes, Pred=No\nSample 1150: True=Yes, Pred=No\nSample 1160: True=Yes, Pred=No\nSample 1170: True=No, Pred=No\nSample 1180: True=No, Pred=No\nSample 1190: True=No, Pred=No\nSample 1200: True=No, Pred=No\nSample 1210: True=No, Pred=No\nSample 1220: True=No, Pred=No\nSample 1230: True=No, Pred=No\nSample 1240: True=No, Pred=No\nSample 1250: True=No, Pred=No\nSample 1260: True=No, Pred=No\nSample 1270: True=No, Pred=No\nSample 1280: True=No, Pred=No\nSample 1290: True=No, Pred=No\nSample 1300: True=Yes, Pred=No\nSample 1310: True=No, Pred=No\nSample 1320: True=No, Pred=No\nSample 1330: True=No, Pred=No\nSample 1340: True=Yes, Pred=No\nSample 1350: True=No, Pred=No\nSample 1360: True=No, Pred=No\nSample 1370: True=No, Pred=No\nSample 1380: True=No, Pred=No\nSample 1390: True=No, Pred=No\nSample 1400: True=No, Pred=No\nSample 1410: True=No, Pred=No\nSample 1420: True=Yes, Pred=No\nSample 1430: True=No, Pred=No\nSample 1440: True=No, Pred=No\nSample 1450: True=No, Pred=No\nSample 1460: True=No, Pred=No\nSample 1470: True=No, Pred=No\nSample 1480: True=Yes, Pred=No\nSample 1490: True=No, Pred=No\nSample 1500: True=No, Pred=No\nSample 1510: True=No, Pred=No\nSample 1520: True=No, Pred=No\nSample 1530: True=No, Pred=No\nSample 1540: True=Yes, Pred=No\nSample 1550: True=No, Pred=No\nSample 1560: True=No, Pred=No\nSample 1570: True=No, Pred=No\nSample 1580: True=No, Pred=No\nSample 1590: True=No, Pred=No\nSample 1600: True=No, Pred=No\nSample 1610: True=No, Pred=No\nSample 1620: True=No, Pred=No\nSample 1630: True=No, Pred=No\nSample 1640: True=No, Pred=No\nSample 1650: True=No, Pred=No\nSample 1660: True=No, Pred=No\nSample 1670: True=No, Pred=No\nSample 1680: True=No, Pred=No\nSample 1690: True=Yes, Pred=No\nSample 1700: True=No, Pred=No\nSample 1710: True=No, Pred=No\nSample 1720: True=No, Pred=No\nSample 1730: True=No, Pred=No\nSample 1740: True=No, Pred=No\nSample 1750: True=No, Pred=No\nSample 1760: True=No, Pred=No\nSample 1770: True=No, Pred=No\nSample 1780: True=No, Pred=No\nSample 1790: True=No, Pred=No\nSample 1800: True=No, Pred=No\nSample 1810: True=No, Pred=No\nSample 1820: True=No, Pred=No\nSample 1830: True=No, Pred=No\nSample 1840: True=No, Pred=No\nSample 1850: True=No, Pred=No\nSample 1860: True=No, Pred=No\nSample 1870: True=No, Pred=No\nSample 1880: True=No, Pred=No\nSample 1890: True=No, Pred=No\nSample 1900: True=No, Pred=No\nSample 1910: True=No, Pred=No\nSample 1920: True=No, Pred=No\nSample 1930: True=No, Pred=No\nSample 1940: True=No, Pred=No\nSample 1950: True=No, Pred=No\nSample 1960: True=No, Pred=No\nSample 1970: True=No, Pred=No\nSample 1980: True=No, Pred=No\nSample 1990: True=No, Pred=No\nSample 2000: True=No, Pred=No\nSample 2010: True=No, Pred=No\nSample 2020: True=No, Pred=No\nSample 2030: True=No, Pred=No\nSample 2040: True=No, Pred=No\nSample 2050: True=No, Pred=No\nSample 2060: True=No, Pred=No\nSample 2070: True=No, Pred=No\nSample 2080: True=Yes, Pred=No\nSample 2090: True=No, Pred=No\nSample 2100: True=Yes, Pred=No\nSample 2110: True=No, Pred=No\nSample 2120: True=No, Pred=No\nSample 2130: True=No, Pred=No\nSample 2140: True=No, Pred=No\nSample 2150: True=Yes, Pred=No\nSample 2160: True=No, Pred=No\nSample 2170: True=No, Pred=No\nSample 2180: True=No, Pred=No\nSample 2190: True=No, Pred=No\nSample 2200: True=No, Pred=No\nSample 2210: True=No, Pred=No\nSample 2220: True=No, Pred=No\nSample 2230: True=Yes, Pred=No\nSample 2240: True=No, Pred=No\nSample 2250: True=Yes, Pred=No\nSample 2260: True=No, Pred=No\nSample 2270: True=No, Pred=No\nSample 2280: True=No, Pred=No\nSample 2290: True=No, Pred=No\nSample 2300: True=Yes, Pred=No\nSample 2310: True=No, Pred=No\nSample 2320: True=No, Pred=No\nSample 2330: True=No, Pred=No\nSample 2340: True=No, Pred=No\nSample 2350: True=No, Pred=No\nSample 2360: True=No, Pred=No\nSample 2370: True=No, Pred=No\nSample 2380: True=No, Pred=No\nSample 2390: True=No, Pred=No\nSample 2400: True=No, Pred=No\nSample 2410: True=Yes, Pred=No\nSample 2420: True=No, Pred=No\nSample 2430: True=No, Pred=No\nSample 2440: True=No, Pred=No\nSample 2450: True=No, Pred=No\nSample 2460: True=Yes, Pred=No\nSample 2470: True=No, Pred=No\nSample 2480: True=No, Pred=No\nSample 2490: True=No, Pred=No\nSample 2500: True=No, Pred=No\nSample 2510: True=No, Pred=No\nSample 2520: True=No, Pred=No\nSample 2530: True=No, Pred=No\nSample 2540: True=No, Pred=No\nSample 2550: True=No, Pred=No\nSample 2560: True=No, Pred=No\nSample 2570: True=No, Pred=No\nSample 2580: True=No, Pred=No\nSample 2590: True=No, Pred=No\nSample 2600: True=No, Pred=No\nSample 2610: True=No, Pred=No\nSample 2620: True=No, Pred=No\nSample 2630: True=No, Pred=No\nSample 2640: True=No, Pred=No\nSample 2650: True=No, Pred=No\nSample 2660: True=No, Pred=No\nSample 2670: True=No, Pred=No\nSample 2680: True=No, Pred=No\nSample 2690: True=No, Pred=No\nSample 2700: True=No, Pred=No\nSample 2710: True=No, Pred=No\nSample 2720: True=No, Pred=No\nSample 2730: True=No, Pred=No\nSample 2740: True=No, Pred=No\nSample 2750: True=No, Pred=No\nSample 2760: True=Yes, Pred=No\nSample 2770: True=Yes, Pred=No\nSample 2780: True=No, Pred=No\nSample 2790: True=No, Pred=No\nSample 2800: True=No, Pred=No\nSample 2810: True=No, Pred=No\nSample 2820: True=No, Pred=No\nSample 2830: True=No, Pred=No\nSample 2840: True=No, Pred=No\nSample 2850: True=Yes, Pred=No\nSample 2860: True=No, Pred=No\nSample 2870: True=No, Pred=No\nSample 2880: True=No, Pred=No\nSample 2890: True=No, Pred=No\nSample 2900: True=No, Pred=No\nSample 2910: True=No, Pred=No\nSample 2920: True=Yes, Pred=No\nSample 2930: True=No, Pred=No\nSample 2940: True=No, Pred=No\nSample 2950: True=No, Pred=No\nSample 2960: True=No, Pred=No\nSample 2970: True=No, Pred=No\nSample 2980: True=No, Pred=No\nSample 2990: True=No, Pred=No\nSample 3000: True=No, Pred=No\nSample 3010: True=No, Pred=No\nSample 3020: True=No, Pred=No\nSample 3030: True=No, Pred=No\nSample 3040: True=Yes, Pred=No\nSample 3050: True=No, Pred=No\nSample 3060: True=No, Pred=No\nSample 3070: True=No, Pred=No\nSample 3080: True=No, Pred=No\nSample 3090: True=No, Pred=No\nSample 3100: True=No, Pred=No\nSample 3110: True=No, Pred=No\nSample 3120: True=Yes, Pred=No\nSample 3130: True=No, Pred=No\nSample 3140: True=No, Pred=No\nSample 3150: True=Yes, Pred=No\nSample 3160: True=No, Pred=No\nSample 3170: True=No, Pred=No\nSample 3180: True=No, Pred=No\nSample 3190: True=No, Pred=No\nSample 3200: True=No, Pred=No\nSample 3210: True=No, Pred=No\nSample 3220: True=No, Pred=No\nSample 3230: True=No, Pred=No\nSample 3240: True=Yes, Pred=No\nSample 3250: True=No, Pred=No\nSample 3260: True=No, Pred=No\nSample 3270: True=No, Pred=No\nSample 3280: True=No, Pred=No\nSample 3290: True=No, Pred=No\nSample 3300: True=No, Pred=No\nSample 3310: True=No, Pred=No\nSample 3320: True=No, Pred=No\nSample 3330: True=No, Pred=No\nSample 3340: True=No, Pred=No\nSample 3350: True=No, Pred=No\nSample 3360: True=No, Pred=No\nSample 3370: True=No, Pred=No\nSample 3380: True=No, Pred=No\nSample 3390: True=No, Pred=No\nSample 3400: True=No, Pred=No\nSample 3410: True=No, Pred=No\nSample 3420: True=No, Pred=No\nSample 3430: True=No, Pred=No\nSample 3440: True=No, Pred=No\nSample 3450: True=No, Pred=No\nSample 3460: True=No, Pred=No\nSample 3470: True=No, Pred=No\nSample 3480: True=No, Pred=No\nSample 3490: True=No, Pred=No\nSample 3500: True=No, Pred=No\nSample 3510: True=No, Pred=No\nSample 3520: True=No, Pred=No\nSample 3530: True=No, Pred=No\nSample 3540: True=No, Pred=No\nSample 3550: True=No, Pred=No\nSample 3560: True=No, Pred=No\nSample 3570: True=No, Pred=No\nSample 3580: True=Yes, Pred=No\nSample 3590: True=No, Pred=No\nSample 3600: True=No, Pred=No\nSample 3610: True=No, Pred=No\nSample 3620: True=No, Pred=No\nSample 3630: True=No, Pred=No\nSample 3640: True=Yes, Pred=No\nSample 3650: True=No, Pred=No\nSample 3660: True=No, Pred=No\nSample 3670: True=No, Pred=No\nSample 3680: True=No, Pred=No\nSample 3690: True=No, Pred=No\nSample 3700: True=No, Pred=No\nSample 3710: True=Yes, Pred=No\nSample 3720: True=No, Pred=No\nSample 3730: True=No, Pred=No\nSample 3740: True=No, Pred=No\nSample 3750: True=Yes, Pred=No\nSample 3760: True=No, Pred=No\nSample 3770: True=No, Pred=No\nSample 3780: True=No, Pred=No\nSample 3790: True=No, Pred=No\nSample 3800: True=No, Pred=No\nSample 3810: True=No, Pred=No\nSample 3820: True=No, Pred=No\nSample 3830: True=No, Pred=No\nSample 3840: True=No, Pred=No\nSample 3850: True=No, Pred=No\nSample 3860: True=No, Pred=No\nSample 3870: True=Yes, Pred=No\nSample 3880: True=No, Pred=No\nSample 3890: True=No, Pred=No\nSample 3900: True=No, Pred=No\nSample 3910: True=No, Pred=No\nSample 3920: True=No, Pred=No\nSample 3930: True=No, Pred=No\nSample 3940: True=Yes, Pred=No\nSample 3950: True=No, Pred=No\nSample 3960: True=No, Pred=No\nSample 3970: True=No, Pred=No\nSample 3980: True=No, Pred=No\nSample 3990: True=Yes, Pred=No\nSample 4000: True=No, Pred=No\nSample 4010: True=No, Pred=No\nSample 4020: True=No, Pred=No\nSample 4030: True=No, Pred=No\nSample 4040: True=No, Pred=No\nSample 4050: True=No, Pred=No\nSample 4060: True=No, Pred=No\nSample 4070: True=No, Pred=No\nSample 4080: True=No, Pred=No\nSample 4090: True=No, Pred=No\nSample 4100: True=No, Pred=No\nSample 4110: True=No, Pred=No\nSample 4120: True=No, Pred=No\nSample 4130: True=No, Pred=No\nSample 4140: True=Yes, Pred=No\nSample 4150: True=No, Pred=No\nSample 4160: True=No, Pred=No\nSample 4170: True=No, Pred=No\nSample 4180: True=No, Pred=No\nSample 4190: True=No, Pred=No\nSample 4200: True=Yes, Pred=No\nSample 4210: True=No, Pred=No\nSample 4220: True=No, Pred=No\nSample 4230: True=No, Pred=No\nSample 4240: True=No, Pred=No\nSample 4250: True=No, Pred=No\nSample 4260: True=No, Pred=No\nSample 4270: True=No, Pred=No\nSample 4280: True=No, Pred=No\nSample 4290: True=No, Pred=No\nSample 4300: True=No, Pred=No\nSample 4310: True=No, Pred=No\nSample 4320: True=No, Pred=No\nSample 4330: True=No, Pred=No\nSample 4340: True=No, Pred=No\nSample 4350: True=No, Pred=No\nSample 4360: True=Yes, Pred=No\nSample 4370: True=No, Pred=No\nSample 4380: True=No, Pred=No\nSample 4390: True=No, Pred=No\nSample 4400: True=No, Pred=No\nSample 4410: True=No, Pred=No\nSample 4420: True=Yes, Pred=No\nSample 4430: True=No, Pred=No\nSample 4440: True=No, Pred=No\nSample 4450: True=No, Pred=No\nSample 4460: True=No, Pred=No\nSample 4470: True=No, Pred=No\nSample 4480: True=No, Pred=No\nSample 4490: True=No, Pred=No\nSample 4500: True=No, Pred=No\nSample 4510: True=No, Pred=No\nSample 4520: True=No, Pred=No\nSample 4530: True=No, Pred=No\nSample 4540: True=No, Pred=No\nSample 4550: True=Yes, Pred=No\nSample 4560: True=No, Pred=No\nSample 4570: True=No, Pred=No\nSample 4580: True=No, Pred=No\nSample 4590: True=No, Pred=No\nSample 4600: True=Yes, Pred=No\nSample 4610: True=No, Pred=No\nSample 4620: True=No, Pred=No\nSample 4630: True=No, Pred=No\nSample 4640: True=No, Pred=No\nSample 4650: True=No, Pred=No\nSample 4660: True=No, Pred=No\nSample 4670: True=No, Pred=No\nSample 4680: True=No, Pred=No\nSample 4690: True=No, Pred=No\nSample 4700: True=No, Pred=No\nSample 4710: True=No, Pred=No\nSample 4720: True=No, Pred=No\nSample 4730: True=No, Pred=No\nSample 4740: True=No, Pred=No\nSample 4750: True=No, Pred=No\nSample 4760: True=No, Pred=No\nSample 4770: True=No, Pred=No\nSample 4780: True=No, Pred=No\nSample 4790: True=No, Pred=No\nSample 4800: True=No, Pred=No\nSample 4810: True=Yes, Pred=No\nSample 4820: True=No, Pred=No\nSample 4830: True=No, Pred=No\nSample 4840: True=No, Pred=No\nSample 4850: True=No, Pred=No\nSample 4860: True=No, Pred=No\nSample 4870: True=No, Pred=No\nSample 4880: True=No, Pred=No\nSample 4890: True=Yes, Pred=No\nSample 4900: True=No, Pred=No\nSample 4910: True=Yes, Pred=No\nSample 4920: True=No, Pred=No\nSample 4930: True=No, Pred=No\nSample 4940: True=No, Pred=No\nSample 4950: True=Yes, Pred=No\nSample 4960: True=No, Pred=No\nSample 4970: True=No, Pred=No\nSample 4980: True=No, Pred=No\nSample 4990: True=No, Pred=No\nSample 5000: True=No, Pred=No\nSample 5010: True=No, Pred=No\nSample 5020: True=Yes, Pred=No\nSample 5030: True=No, Pred=No\nSample 5040: True=No, Pred=No\nSample 5050: True=No, Pred=No\nSample 5060: True=No, Pred=No\nSample 5070: True=No, Pred=No\nSample 5080: True=No, Pred=No\nSample 5090: True=No, Pred=No\nSample 5100: True=No, Pred=No\nSample 5110: True=No, Pred=No\nSample 5120: True=Yes, Pred=No\nSample 5130: True=No, Pred=No\nSample 5140: True=No, Pred=No\nSample 5150: True=No, Pred=No\nSample 5160: True=No, Pred=No\nSample 5170: True=No, Pred=No\nSample 5180: True=No, Pred=No\nSample 5190: True=No, Pred=No\nSample 5200: True=Yes, Pred=No\nSample 5210: True=Yes, Pred=No\nSample 5220: True=No, Pred=No\nSample 5230: True=No, Pred=No\nSample 5240: True=No, Pred=No\nSample 5250: True=No, Pred=No\nSample 5260: True=No, Pred=No\nSample 5270: True=No, Pred=No\nSample 5280: True=No, Pred=No\nSample 5290: True=No, Pred=No\nSample 5300: True=No, Pred=No\nSample 5310: True=No, Pred=No\nSample 5320: True=No, Pred=No\nSample 5330: True=No, Pred=No\nSample 5340: True=No, Pred=No\nSample 5350: True=No, Pred=No\nSample 5360: True=No, Pred=No\nSample 5370: True=No, Pred=No\nSample 5380: True=No, Pred=No\nSample 5390: True=No, Pred=No\nSample 5400: True=Yes, Pred=No\nSample 5410: True=No, Pred=No\nSample 5420: True=No, Pred=No\nSample 5430: True=No, Pred=No\nSample 5440: True=No, Pred=No\nSample 5450: True=No, Pred=No\nSample 5460: True=No, Pred=No\nSample 5470: True=No, Pred=No\nSample 5480: True=No, Pred=No\nSample 5490: True=No, Pred=No\nSample 5500: True=No, Pred=No\nSample 5510: True=No, Pred=No\nSample 5520: True=No, Pred=No\nSample 5530: True=Yes, Pred=No\nSample 5540: True=No, Pred=No\nSample 5550: True=No, Pred=No\nSample 5560: True=No, Pred=No\nSample 5570: True=No, Pred=No\nSample 5580: True=No, Pred=No\nSample 5590: True=No, Pred=No\nSample 5600: True=No, Pred=No\nSample 5610: True=No, Pred=No\nSample 5620: True=No, Pred=No\nSample 5630: True=No, Pred=No\nSample 5640: True=No, Pred=No\nSample 5650: True=No, Pred=No\nSample 5660: True=No, Pred=No\nSample 5670: True=No, Pred=No\nSample 5680: True=No, Pred=No\nSample 5690: True=No, Pred=No\nSample 5700: True=No, Pred=No\nSample 5710: True=Yes, Pred=No\nSample 5720: True=No, Pred=No\nSample 5730: True=Yes, Pred=No\nSample 5740: True=No, Pred=No\nSample 5750: True=Yes, Pred=No\nSample 5760: True=No, Pred=No\nSample 5770: True=No, Pred=No\nSample 5780: True=No, Pred=No\nSample 5790: True=No, Pred=No\nSample 5800: True=No, Pred=No\nSample 5810: True=No, Pred=No\nSample 5820: True=No, Pred=No\nSample 5830: True=Yes, Pred=No\nSample 5840: True=No, Pred=No\nSample 5850: True=Yes, Pred=No\nSample 5860: True=No, Pred=No\nSample 5870: True=No, Pred=No\nSample 5880: True=No, Pred=No\nSample 5890: True=No, Pred=No\nSample 5900: True=No, Pred=No\nSample 5910: True=No, Pred=No\nSample 5920: True=No, Pred=No\nSample 5930: True=No, Pred=No\nSample 5940: True=No, Pred=No\nSample 5950: True=No, Pred=No\nSample 5960: True=No, Pred=No\nSample 5970: True=Yes, Pred=No\nSample 5980: True=No, Pred=No\nSample 5990: True=No, Pred=No\nSample 6000: True=No, Pred=No\nSample 6010: True=No, Pred=No\nSample 6020: True=No, Pred=No\nSample 6030: True=No, Pred=No\nSample 6040: True=No, Pred=No\nSample 6050: True=No, Pred=No\nSample 6060: True=No, Pred=No\nSample 6070: True=No, Pred=No\nSample 6080: True=No, Pred=No\nSample 6090: True=No, Pred=No\nSample 6100: True=Yes, Pred=No\nSample 6110: True=No, Pred=No\nSample 6120: True=No, Pred=No\nSample 6130: True=No, Pred=No\nSample 6140: True=No, Pred=No\nSample 6150: True=No, Pred=No\nSample 6160: True=No, Pred=No\nSample 6170: True=No, Pred=No\nSample 6180: True=No, Pred=No\nSample 6190: True=No, Pred=No\nSample 6200: True=No, Pred=No\nSample 6210: True=No, Pred=No\nSample 6220: True=No, Pred=No\nSample 6230: True=No, Pred=No\nSample 6240: True=No, Pred=No\nSample 6250: True=No, Pred=No\nSample 6260: True=No, Pred=No\nSample 6270: True=No, Pred=No\nSample 6280: True=No, Pred=No\nSample 6290: True=No, Pred=No\nSample 6300: True=No, Pred=No\nSample 6310: True=No, Pred=No\nSample 6320: True=No, Pred=No\nSample 6330: True=No, Pred=No\nSample 6340: True=No, Pred=No\nSample 6350: True=Yes, Pred=No\nSample 6360: True=No, Pred=No\nSample 6370: True=No, Pred=No\nSample 6380: True=No, Pred=No\nSample 6390: True=No, Pred=No\nSample 6400: True=Yes, Pred=No\nSample 6410: True=No, Pred=No\nSample 6420: True=No, Pred=No\nSample 6430: True=No, Pred=No\nSample 6440: True=No, Pred=No\nSample 6450: True=No, Pred=No\nSample 6460: True=Yes, Pred=No\nSample 6470: True=No, Pred=No\nSample 6480: True=No, Pred=No\nSample 6490: True=No, Pred=No\nSample 6500: True=No, Pred=No\nSample 6510: True=No, Pred=No\nSample 6520: True=No, Pred=No\nSample 6530: True=Yes, Pred=No\nSample 6540: True=No, Pred=No\nSample 6550: True=No, Pred=No\nSample 6560: True=No, Pred=No\nSample 6570: True=No, Pred=No\nSample 6580: True=No, Pred=No\nSample 6590: True=No, Pred=No\nSample 6600: True=No, Pred=No\nSample 6610: True=No, Pred=No\nSample 6620: True=No, Pred=No\nSample 6630: True=No, Pred=No\nSample 6640: True=No, Pred=No\nSample 6650: True=Yes, Pred=No\nSample 6660: True=No, Pred=No\nSample 6670: True=No, Pred=No\nSample 6680: True=No, Pred=No\nSample 6690: True=No, Pred=No\nSample 6700: True=Yes, Pred=No\nSample 6710: True=No, Pred=No\nSample 6720: True=No, Pred=No\nSample 6730: True=No, Pred=No\nSample 6740: True=No, Pred=No\nSample 6750: True=No, Pred=No\nSample 6760: True=No, Pred=No\nSample 6770: True=No, Pred=No\nSample 6780: True=No, Pred=No\nSample 6790: True=No, Pred=No\nSample 6800: True=No, Pred=No\nSample 6810: True=No, Pred=No\nSample 6820: True=No, Pred=No\nSample 6830: True=No, Pred=No\nSample 6840: True=No, Pred=No\nSample 6850: True=No, Pred=No\nSample 6860: True=Yes, Pred=No\nSample 6870: True=No, Pred=No\nSample 6880: True=No, Pred=No\nSample 6890: True=No, Pred=No\nSample 6900: True=No, Pred=No\nSample 6910: True=No, Pred=No\nSample 6920: True=Yes, Pred=No\nSample 6930: True=No, Pred=No\nSample 6940: True=No, Pred=No\nSample 6950: True=No, Pred=No\nSample 6960: True=No, Pred=No\nSample 6970: True=No, Pred=No\nSample 6980: True=No, Pred=No\nSample 6990: True=No, Pred=No\nSample 7000: True=Yes, Pred=No\nSample 7010: True=No, Pred=No\nSample 7020: True=No, Pred=No\nSample 7030: True=No, Pred=No\nSample 7040: True=No, Pred=No\nSample 7050: True=No, Pred=No\nSample 7060: True=No, Pred=No\nSample 7070: True=No, Pred=No\nSample 7080: True=No, Pred=No\nSample 7090: True=Yes, Pred=No\nSample 7100: True=No, Pred=No\nSample 7110: True=Yes, Pred=No\nSample 7120: True=No, Pred=No\nSample 7130: True=No, Pred=No\nSample 7140: True=No, Pred=No\n\n=== Test Set Metrics ===\nAccuracy: 0.9071\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/train.py\", line 320, in <module>\n[rank0]:     trainer.fit(model, train_loader, valid_loader)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/trainer.py\", line 584, in fit\n[rank0]:     call._call_and_handle_interrupt(\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n[rank0]:     return function(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/trainer.py\", line 630, in _fit_impl\n[rank0]:     self._run(model, ckpt_path=ckpt_path, weights_only=weights_only)\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1079, in _run\n[rank0]:     results = self._run_stage()\n[rank0]:               ^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1123, in _run_stage\n[rank0]:     self.fit_loop.run()\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 224, in run\n[rank0]:     self.on_run_end()\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 507, in on_run_end\n[rank0]:     call._call_lightning_module_hook(trainer, \"on_train_end\")\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/call.py\", line 177, in _call_lightning_module_hook\n[rank0]:     output = fn(*args, **kwargs)\n[rank0]:              ^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/train.py\", line 258, in on_train_end\n[rank0]:     roc = roc_auc_score(trues, preds)\n[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n[rank0]:     return func(*args, **kwargs)\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\", line 620, in roc_auc_score\n[rank0]:     y_score = check_array(y_score, ensure_2d=False)\n[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]:   File \"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\", line 1096, in check_array\n[rank0]:     raise ValueError(\n[rank0]: ValueError: dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.\n","output_type":"stream"}],"execution_count":4}]}