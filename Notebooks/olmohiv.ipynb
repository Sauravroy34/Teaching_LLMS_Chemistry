{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n\n    # Optional: print the name of each GPU\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"CUDA is not available. No GPUs found.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-31T16:57:59.367605Z","iopub.execute_input":"2026-01-31T16:57:59.368115Z","iopub.status.idle":"2026-01-31T16:57:59.372889Z","shell.execute_reply.started":"2026-01-31T16:57:59.368087Z","shell.execute_reply":"2026-01-31T16:57:59.372250Z"}},"outputs":[{"name":"stdout","text":"Number of available GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"! pip install bitsandbytes\n! pip install peft\n! pip install --pre deepchem\n! pip install ai2-olmo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T16:48:07.511593Z","iopub.execute_input":"2026-01-31T16:48:07.511843Z","iopub.status.idle":"2026-01-31T16:48:38.713817Z","shell.execute_reply.started":"2026-01-31T16:48:07.511821Z","shell.execute_reply":"2026-01-31T16:48:38.712934Z"}},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.49.1\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\nCollecting deepchem\n  Downloading deepchem-2.8.1.dev20260121224927-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.5.3)\nCollecting numpy<2 (from deepchem)\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from deepchem) (2.2.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.6.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.13.3)\nRequirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.12/dist-packages (from deepchem) (1.15.3)\nCollecting rdkit (from deepchem)\n  Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->deepchem) (2025.3)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit->deepchem) (11.3.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->deepchem) (3.6.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->deepchem) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->deepchem) (1.17.0)\nDownloading deepchem-2.8.1.dev20260121224927-py3-none-any.whl (1.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl (36.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: numpy, rdkit, deepchem\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepchem-2.8.1.dev20260121224927 numpy-1.26.4 rdkit-2025.9.4\nCollecting ai2-olmo\n  Downloading ai2_olmo-0.6.0-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (1.26.4)\nRequirement already satisfied: torch>=2.1 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.8.0+cu126)\nCollecting ai2-olmo-core==0.1.0 (from ai2-olmo)\n  Downloading ai2_olmo_core-0.1.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.3.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (14.2.0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (1.42.10)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (2.19.0)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (0.22.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (25.0)\nCollecting cached_path>=1.6.2 (from ai2-olmo)\n  Downloading cached_path-1.8.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (4.57.1)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from ai2-olmo) (6.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (0.6.2)\nRequirement already satisfied: pydantic<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.12.5)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ai2-olmo-core==0.1.0->ai2-olmo) (2.32.5)\nCollecting rich (from ai2-olmo)\n  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock<4.0,>=3.4 in /usr/local/lib/python3.12/dist-packages (from cached_path>=1.6.2->ai2-olmo) (3.20.1)\nRequirement already satisfied: huggingface-hub<2.0,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from cached_path>=1.6.2->ai2-olmo) (0.36.0)\nRequirement already satisfied: botocore<1.43.0,>=1.42.10 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (1.42.10)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (1.0.1)\nRequirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from boto3->ai2-olmo) (0.16.0)\nRequirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.38.0)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.26.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage->ai2-olmo) (1.7.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->ai2-olmo) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->ai2-olmo) (2.19.2)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->ai2-olmo) (3.4.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->ai2-olmo) (4.9.3)\nRequirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf->ai2-olmo) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->ai2-olmo) (2025.11.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->ai2-olmo) (4.67.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (2.9.0.post0)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (2.6.2)\nRequirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.71.0)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (5.29.5)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->ai2-olmo) (1.26.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (4.9.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.8.1->cached_path>=1.6.2->ai2-olmo) (1.2.1rc0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->ai2-olmo) (0.1.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=2.0->ai2-olmo-core==0.1.0->ai2-olmo) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (3.11)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ai2-olmo-core==0.1.0->ai2-olmo) (2025.11.12)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1->ai2-olmo) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1->ai2-olmo) (3.0.3)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->ai2-olmo) (0.6.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.43.0,>=1.42.10->boto3->ai2-olmo) (1.17.0)\nDownloading ai2_olmo-0.6.0-py3-none-any.whl (144.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.9/144.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ai2_olmo_core-0.1.0-py3-none-any.whl (56 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cached_path-1.8.1-py3-none-any.whl (37 kB)\nDownloading rich-13.9.4-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: rich, cached_path, ai2-olmo-core, ai2-olmo\n  Attempting uninstall: rich\n    Found existing installation: rich 14.2.0\n    Uninstalling rich-14.2.0:\n      Successfully uninstalled rich-14.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ai2-olmo-0.6.0 ai2-olmo-core-0.1.0 cached_path-1.8.1 rich-13.9.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%%writefile train.py\nimport torch\nimport pytorch_lightning as pl\nimport deepchem as dc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom pytorch_lightning.callbacks import ModelCheckpoint \nfrom deepchem.molnet import load_hiv \nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport seaborn as sns\nimport re\nfrom torch.nn.functional import softmax\n\nclass OlmoDataset(Dataset):\n    def __init__(self, mode=\"Train\", max_length=300):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        tasks, datasets, transformers = load_hiv(featurizer=\"raw\", splitter='scaffold')#scaffold split\n        train, valid, test = datasets\n        \n        self.task_names = tasks#dataset contains a lot of tasks \n\n        self.mode = mode.lower()\n        if self.mode == \"train\":\n            self.data = train\n        elif self.mode == \"valid\":\n            self.data = valid\n        elif self.mode == \"test\":\n            self.data = test\n\n        self.max_length = max_length\n        self.samples = []\n        self._filldataset()\n\n    def _filldataset(self):\n        for i in range(len(self.data)):\n            smiles = self.data.ids[i]\n            labels = self.data.y[i]   \n            weights = self.data.w[i]\n\n            for task_idx, label in enumerate(labels):\n                if weights[task_idx] > 0:\n                    task_name = self.task_names[task_idx]\n                    self.samples.append(self._create_prompt(smiles, task_name, label))\n                    \n        print(f\"[{self.mode.upper()}] Number of samples: {len(self.samples)}\")\n\n    def _create_prompt(self, smiles, task_name, label):\n        eos_token = self.tokenizer.eos_token\n        answer = \"Yes\" if label == 1.0 else \"No\"\n        \n        full_prompt = (\n            \"### Instruction:\\n\"\n            f\"Is the following molecule active against HIV?\\n\"\n            f\"{smiles}\\n\\n\"\n            \"### Response:\\n\"\n            f\"{answer}{eos_token}\"\n        )\n        return full_prompt\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        text = self.samples[idx]\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        input_ids = encodings[\"input_ids\"].squeeze(0)\n        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n        labels = input_ids.clone()\n\n        separator = \"### Response:\\n\"\n        parts = text.split(separator)\n\n        if len(parts) >= 2:\n            prompt_text = parts[0] + separator\n            prompt_encodings = self.tokenizer(\n                prompt_text,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            prompt_len = prompt_encodings[\"input_ids\"].shape[1]\n\n            if prompt_len < len(labels):\n                labels[:prompt_len] = -100\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\nclass OLMO_QLoRA(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.peft_config = LoraConfig(\n            r=32,\n            lora_alpha=64,\n            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM \n        )\n\n    def configure_model(self):\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            quantization_config=self.bnb_config,\n            trust_remote_code=True,\n        )\n        self.model = prepare_model_for_kbit_training(self.model)\n        self.model = get_peft_model(self.model, self.peft_config)\n        self.model.print_trainable_parameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n    \n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        self.log(\"Train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n        return loss\n\n\n    def on_train_end(self):\n        if self.trainer.is_global_zero:\n            print(\"\\nStarting test set evaluation (Accuracy & ROC-AUC)...\")\n            \n            test_dataset = OlmoDataset(mode=\"test\", max_length=300)\n            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n            \n            yes_token_id = self.tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n            no_token_id = self.tokenizer.encode(\"No\", add_special_tokens=False)[0]\n            \n            self.model.eval()\n            \n            y_true = []\n            y_probs = [] \n            y_pred = []  \n\n            print(f\"Evaluating on {len(test_loader)} samples using Logits...\")\n\n            with torch.no_grad():\n                for i, batch in enumerate(test_loader):\n                    batch = {k: v.to(self.device) for k, v in batch.items()}\n                    input_ids = batch[\"input_ids\"]\n                    labels = batch[\"labels\"]\n                    attention_mask = batch[\"attention_mask\"]\n\n                    # Find where the prompt ends\n                    response_mask = (labels != -100)\n                    if response_mask.sum() == 0: continue\n                    \n                    answer_start_index = response_mask.int().argmax(dim=1).item()\n                    \n                    if answer_start_index > 0:\n                        prompt_ids = input_ids[:, :answer_start_index]\n                        prompt_mask = attention_mask[:, :answer_start_index]\n                    else:\n                        continue \n\n                    outputs = self.model(input_ids=prompt_ids, attention_mask=prompt_mask)\n                    next_token_logits = outputs.logits[:, -1, :]\n                    \n                    yes_logit = next_token_logits[:, yes_token_id]\n                    no_logit = next_token_logits[:, no_token_id]\n                    \n                    relevant_logits = torch.stack([no_logit, yes_logit], dim=1)\n                    probs = softmax(relevant_logits, dim=1)\n                    \n                    prob_yes = probs[:, 1].item()\n                    prediction = 1 if prob_yes > 0.5 else 0\n                    \n                    # Get True Label\n                    truth_id = labels[0][answer_start_index].item()\n                    if truth_id == yes_token_id:\n                        true_label = 1\n                    elif truth_id == no_token_id:\n                        true_label = 0\n                    else:\n                        continue \n\n                    y_true.append(true_label)\n                    y_probs.append(prob_yes)\n                    y_pred.append(prediction)\n\n                    if i % 100 == 0:\n                        print(f\"Sample {i}: True={true_label}, Prob(Yes)={prob_yes:.4f}\")\n\n            acc = accuracy_score(y_true, y_pred)\n            roc = roc_auc_score(y_true, y_probs)\n\n\n            print(\"\\n=== Test Set Metrics ===\")\n            print(f\"Accuracy: {acc:.4f}\")\n            print(f\"ROC-AUC:  {roc:.4f}\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4,weight_decay=0.001)\n        \n        total_steps = self.trainer.estimated_stepping_batches\n        \n        warmup_steps = int(0.10*total_steps)\n        scheduler_warmup = LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=warmup_steps)\n        scheduler_cosine = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps)\n        scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[warmup_steps])\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n\nif __name__ == \"__main__\":\n    dataset = OlmoDataset()\n    train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n\n    trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            devices=2,\n            strategy=\"ddp\",\n            max_epochs=1, \n            precision=\"16-mixed\",\n            accumulate_grad_batches=8, \n            gradient_clip_val=0.5,\n            enable_progress_bar=False,\n            enable_checkpointing=False,\n\n    )\n    \n    model = OLMO_QLoRA()\n    \n    trainer.fit(model, train_loader)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T16:57:31.759442Z","iopub.execute_input":"2026-01-31T16:57:31.760257Z","iopub.status.idle":"2026-01-31T16:57:31.768934Z","shell.execute_reply.started":"2026-01-31T16:57:31.760221Z","shell.execute_reply":"2026-01-31T16:57:31.768392Z"}},"outputs":[{"name":"stdout","text":"Overwriting train.py\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T17:00:09.252591Z","iopub.execute_input":"2026-01-31T17:00:09.253406Z","iopub.status.idle":"2026-01-31T22:20:49.049952Z","shell.execute_reply.started":"2026-01-31T17:00:09.253374Z","shell.execute_reply":"2026-01-31T22:20:49.043139Z"}},"outputs":[{"name":"stdout","text":"No normalization for SPS. Feature removed!\nNo normalization for AvgIpc. Feature removed!\nNo normalization for NumAmideBonds. Feature removed!\nNo normalization for NumAtomStereoCenters. Feature removed!\nNo normalization for NumBridgeheadAtoms. Feature removed!\nNo normalization for NumHeterocycles. Feature removed!\nNo normalization for NumSpiroAtoms. Feature removed!\nNo normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\nNo normalization for Phi. Feature removed!\n2026-01-31 17:00:16.032462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769878816.054711     210 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769878816.061841     210 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769878816.078760     210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769878816.078792     210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769878816.078807     210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769878816.078811     210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nWARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nexperimental_relax_shapes is deprecated, use reduce_retracing instead\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nSkipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\nWARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.12/dist-packages/deepchem/models/torch_models/__init__.py)\nWARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\nWARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n[TRAIN] Number of samples: 32896\nUsing 16bit Automatic Mixed Precision (AMP)\nüí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\nNo normalization for SPS. Feature removed!\nNo normalization for AvgIpc. Feature removed!\nNo normalization for NumAmideBonds. Feature removed!\nNo normalization for NumAtomStereoCenters. Feature removed!\nNo normalization for NumBridgeheadAtoms. Feature removed!\nNo normalization for NumHeterocycles. Feature removed!\nNo normalization for NumSpiroAtoms. Feature removed!\nNo normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\nNo normalization for Phi. Feature removed!\n2026-01-31 17:10:48.916710: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769879448.937537     228 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769879448.943933     228 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769879448.960404     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769879448.960434     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769879448.960440     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769879448.960446     228 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nWARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\nInstructions for updating:\nexperimental_relax_shapes is deprecated, use reduce_retracing instead\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\nSkipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\nWARNING:deepchem.models:Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/usr/local/lib/python3.12/dist-packages/deepchem/models/torch_models/__init__.py)\nWARNING:deepchem.models:Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'lightning'\nWARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n[TRAIN] Number of samples: 32896\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 628/628 [00:00<00:00, 5.42MB/s]\nmodel.safetensors.index.json: 18.4kB [00:00, 44.8MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/3.83G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/4.95G [00:00<?, ?B/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   0%| | 44.4k/5.00G [00:02<76:04:56, 18.2kB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|    | 636k/4.95G [00:02<5:17:31, 260kB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   0%| | 35.2k/3.83G [00:02<76:36:26, 13.9kB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   0%|    | 3.45M/5.00G [00:02<44:31, 1.87MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   0%|    | 9.06M/3.83G [00:02<13:13, 4.81MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   0%|    | 7.20M/5.00G [00:02<18:44, 4.44MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   0%|    | 15.1M/5.00G [00:02<07:29, 11.1MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   1%|    | 36.3M/3.83G [00:02<02:53, 21.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 28.5M/5.00G [00:03<03:45, 22.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   1%|    | 44.8M/3.83G [00:03<03:01, 20.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 35.8M/5.00G [00:03<03:56, 21.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   2%|    | 62.4M/3.83G [00:03<01:51, 33.9MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   2%|    | 75.9M/3.83G [00:03<01:30, 41.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   0%|   | 1.76M/4.95G [00:03<2:34:43, 533kB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 40.0M/5.00G [00:03<04:21, 19.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   2%|    | 90.4M/3.83G [00:03<01:17, 48.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   3%|‚ñè    | 136M/4.95G [00:03<01:19, 60.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:   5%|‚ñé     | 177M/3.83G [00:03<00:22, 159MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 43.0M/5.00G [00:04<05:04, 16.3MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   7%|‚ñç     | 260M/3.83G [00:04<00:16, 220MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 45.7M/5.00G [00:04<04:54, 16.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 48.7M/5.00G [00:04<04:25, 18.6MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:   9%|‚ñå     | 358M/3.83G [00:04<00:13, 248MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 52.0M/5.00G [00:04<06:12, 13.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   4%|‚ñè    | 205M/4.95G [00:04<01:10, 67.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   1%|    | 54.2M/5.00G [00:04<06:28, 12.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   2%|     | 121M/5.00G [00:05<00:55, 87.3MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  10%|‚ñå     | 391M/3.83G [00:05<00:28, 122MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  14%|‚ñä     | 552M/3.83G [00:05<00:13, 243MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   6%|‚ñé    | 272M/4.95G [00:05<01:07, 68.8MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:   7%|‚ñé    | 339M/4.95G [00:06<00:59, 77.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  17%|‚ñâ     | 637M/3.83G [00:06<00:19, 166MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:   8%|‚ñç     | 406M/4.95G [00:06<00:42, 106MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  18%|‚ñà     | 704M/3.83G [00:06<00:15, 196MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  10%|‚ñå     | 473M/4.95G [00:07<00:41, 108MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  11%|‚ñã     | 541M/4.95G [00:07<00:34, 127MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  12%|‚ñã     | 607M/4.95G [00:07<00:30, 142MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|‚ñè    | 133M/5.00G [00:07<04:12, 19.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  14%|‚ñä     | 674M/4.95G [00:08<00:27, 155MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  20%|‚ñà    | 771M/3.83G [00:08<00:32, 92.7MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  22%|‚ñà‚ñé    | 838M/3.83G [00:08<00:24, 121MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  16%|‚ñâ     | 808M/4.95G [00:08<00:21, 194MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  18%|‚ñà     | 875M/4.95G [00:09<00:24, 166MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  20%|‚ñà    | 1.01G/4.95G [00:09<00:17, 225MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|‚ñè    | 140M/5.00G [00:10<07:08, 11.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  23%|‚ñà‚ñè   | 1.14G/4.95G [00:10<00:17, 216MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  25%|‚ñà‚ñè   | 940M/3.83G [00:10<00:34, 84.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  24%|‚ñà‚ñè   | 1.21G/4.95G [00:10<00:16, 230MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  26%|‚ñà   | 1.01G/3.83G [00:11<00:40, 70.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  26%|‚ñà‚ñé   | 1.28G/4.95G [00:11<00:31, 116MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  27%|‚ñà   | 1.34G/4.95G [00:13<00:47, 75.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|‚ñè    | 145M/5.00G [00:13<13:20, 6.06MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  28%|‚ñà   | 1.07G/3.83G [00:14<00:57, 47.7MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  30%|‚ñà‚ñè  | 1.14G/3.83G [00:16<01:02, 42.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  29%|‚ñà‚ñè  | 1.41G/4.95G [00:16<01:17, 45.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  32%|‚ñà‚ñé  | 1.21G/3.83G [00:16<00:50, 51.7MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  33%|‚ñà‚ñé  | 1.25G/3.83G [00:17<00:44, 58.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  31%|‚ñà‚ñè  | 1.55G/4.95G [00:17<00:50, 67.8MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  33%|‚ñà‚ñé  | 1.28G/3.83G [00:17<00:40, 63.3MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  37%|‚ñà‚ñç  | 1.42G/3.83G [00:18<00:24, 98.1MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  39%|‚ñà‚ñâ   | 1.48G/3.83G [00:18<00:22, 103MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  33%|‚ñà‚ñé  | 1.61G/4.95G [00:18<00:53, 62.7MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  40%|‚ñà‚ñà   | 1.55G/3.83G [00:19<00:19, 115MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  42%|‚ñà‚ñà   | 1.62G/3.83G [00:19<00:14, 150MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  44%|‚ñà‚ñà‚ñè  | 1.68G/3.83G [00:20<00:19, 110MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  35%|‚ñà‚ñç  | 1.75G/4.95G [00:21<00:55, 57.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|‚ñè    | 150M/5.00G [00:21<30:23, 2.66MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  46%|‚ñà‚ñä  | 1.75G/3.83G [00:21<00:24, 84.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  38%|‚ñà‚ñå  | 1.88G/4.95G [00:24<00:55, 55.6MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  47%|‚ñà‚ñâ  | 1.82G/3.83G [00:24<00:39, 50.9MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  49%|‚ñà‚ñâ  | 1.89G/3.83G [00:26<00:48, 40.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  39%|‚ñà‚ñå  | 1.95G/4.95G [00:26<01:07, 44.7MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   3%|‚ñè    | 153M/5.00G [00:28<46:33, 1.73MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  51%|‚ñà‚ñà  | 1.95G/3.83G [00:28<00:45, 41.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  41%|‚ñà‚ñã  | 2.02G/4.95G [00:28<01:06, 44.4MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  42%|‚ñà‚ñã  | 2.08G/4.95G [00:28<00:54, 52.9MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  53%|‚ñà‚ñà  | 2.02G/3.83G [00:28<00:35, 50.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   4%|‚ñè    | 224M/5.00G [00:28<10:17, 7.74MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  45%|‚ñà‚ñä  | 2.22G/4.95G [00:29<00:36, 75.3MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  54%|‚ñà‚ñà‚ñè | 2.09G/3.83G [00:29<00:29, 59.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  46%|‚ñà‚ñä  | 2.28G/4.95G [00:29<00:28, 93.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  56%|‚ñà‚ñà‚ñè | 2.15G/3.83G [00:30<00:25, 66.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  48%|‚ñà‚ñâ  | 2.35G/4.95G [00:30<00:26, 96.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 228M/5.00G [00:30<11:14, 7.07MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 233M/5.00G [00:30<10:17, 7.71MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  49%|‚ñà‚ñà‚ñç  | 2.42G/4.95G [00:30<00:21, 118MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  58%|‚ñà‚ñà‚ñé | 2.22G/3.83G [00:31<00:23, 68.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  50%|‚ñà‚ñà‚ñå  | 2.48G/4.95G [00:31<00:23, 104MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 236M/5.00G [00:31<11:25, 6.94MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  52%|‚ñà‚ñà‚ñå  | 2.55G/4.95G [00:31<00:19, 125MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 241M/5.00G [00:31<09:55, 7.99MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  60%|‚ñà‚ñà‚ñç | 2.29G/3.83G [00:32<00:24, 63.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  53%|‚ñà‚ñà  | 2.62G/4.95G [00:32<00:26, 88.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 245M/5.00G [00:33<13:07, 6.03MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  61%|‚ñà‚ñà‚ñç | 2.36G/3.83G [00:34<00:31, 47.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  54%|‚ñà‚ñà‚ñè | 2.68G/4.95G [00:35<00:42, 53.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñè    | 248M/5.00G [00:35<19:58, 3.96MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 251M/5.00G [00:35<17:32, 4.51MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 253M/5.00G [00:35<15:35, 5.07MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 256M/5.00G [00:35<13:37, 5.80MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 258M/5.00G [00:36<12:24, 6.37MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  63%|‚ñà‚ñà‚ñå | 2.42G/3.83G [00:37<00:36, 38.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  56%|‚ñà‚ñà‚ñè | 2.75G/4.95G [00:37<00:53, 41.0MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  65%|‚ñà‚ñà‚ñå | 2.49G/3.83G [00:39<00:36, 36.3MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  57%|‚ñà‚ñà‚ñé | 2.82G/4.95G [00:39<00:51, 41.0MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 260M/5.00G [00:39<34:57, 2.26MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  67%|‚ñà‚ñà‚ñã | 2.56G/3.83G [00:39<00:28, 44.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  58%|‚ñà‚ñà‚ñé | 2.89G/4.95G [00:40<00:41, 50.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 262M/5.00G [00:40<33:48, 2.33MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  68%|‚ñà‚ñà‚ñã | 2.62G/3.83G [00:40<00:21, 54.9MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  60%|‚ñà‚ñà‚ñç | 2.95G/4.95G [00:40<00:32, 60.7MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  70%|‚ñà‚ñà‚ñä | 2.69G/3.83G [00:41<00:17, 64.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  61%|‚ñà‚ñà‚ñç | 3.02G/4.95G [00:41<00:27, 69.9MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 263M/5.00G [00:41<41:46, 1.89MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  72%|‚ñà‚ñà‚ñâ | 2.76G/3.83G [00:41<00:14, 73.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  62%|‚ñà‚ñà‚ñç | 3.09G/4.95G [00:41<00:24, 77.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 264M/5.00G [00:41<44:40, 1.77MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  74%|‚ñà‚ñà‚ñâ | 2.82G/3.83G [00:42<00:12, 79.9MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  64%|‚ñà‚ñà‚ñå | 3.15G/4.95G [00:42<00:21, 82.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 265M/5.00G [00:42<50:21, 1.57MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 272M/5.00G [00:42<17:20, 4.54MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  75%|‚ñà‚ñà‚ñà | 2.89G/3.83G [00:43<00:11, 84.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  65%|‚ñà‚ñà‚ñå | 3.22G/4.95G [00:43<00:20, 85.2MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   5%|‚ñé    | 275M/5.00G [00:43<16:38, 4.73MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 281M/5.00G [00:43<09:37, 8.17MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 285M/5.00G [00:43<07:29, 10.5MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  77%|‚ñà‚ñà‚ñà | 2.96G/3.83G [00:43<00:10, 85.6MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  66%|‚ñà‚ñà‚ñã | 3.29G/4.95G [00:44<00:19, 84.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 288M/5.00G [00:44<10:10, 7.71MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 294M/5.00G [00:44<06:33, 12.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 298M/5.00G [00:44<05:51, 13.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 302M/5.00G [00:44<04:53, 16.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  79%|‚ñà‚ñà‚ñà‚ñè| 3.03G/3.83G [00:44<00:10, 78.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 305M/5.00G [00:44<04:28, 17.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 309M/5.00G [00:45<04:00, 19.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 312M/5.00G [00:45<03:48, 20.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  68%|‚ñà‚ñà‚ñã | 3.36G/4.95G [00:45<00:21, 73.1MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 316M/5.00G [00:45<03:56, 19.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|‚ñé    | 319M/5.00G [00:45<03:46, 20.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 326M/5.00G [00:45<03:02, 25.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 329M/5.00G [00:45<03:36, 21.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 333M/5.00G [00:46<03:59, 19.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 336M/5.00G [00:46<04:39, 16.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 339M/5.00G [00:46<05:08, 15.1MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  81%|‚ñà‚ñà‚ñà‚ñè| 3.09G/3.83G [00:46<00:13, 56.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 341M/5.00G [00:46<05:07, 15.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 345M/5.00G [00:47<05:56, 13.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 348M/5.00G [00:47<05:51, 13.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 352M/5.00G [00:47<05:47, 13.4MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  69%|‚ñà‚ñà‚ñä | 3.42G/4.95G [00:47<00:31, 48.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 354M/5.00G [00:47<05:49, 13.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 356M/5.00G [00:48<05:58, 12.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 359M/5.00G [00:48<05:40, 13.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 360M/5.00G [00:48<05:58, 12.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 364M/5.00G [00:48<05:52, 13.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 367M/5.00G [00:48<05:47, 13.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 368M/5.00G [00:48<05:57, 13.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  82%|‚ñà‚ñà‚ñà‚ñé| 3.16G/3.83G [00:49<00:15, 42.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|‚ñé    | 374M/5.00G [00:49<05:33, 13.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 376M/5.00G [00:49<05:34, 13.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 378M/5.00G [00:49<05:16, 14.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 381M/5.00G [00:49<05:24, 14.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 382M/5.00G [00:49<05:14, 14.7MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  71%|‚ñà‚ñà‚ñä | 3.49G/4.95G [00:50<00:35, 40.6MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 388M/5.00G [00:50<04:54, 15.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 395M/5.00G [00:50<03:30, 21.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 399M/5.00G [00:50<03:22, 22.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 403M/5.00G [00:50<03:35, 21.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 406M/5.00G [00:50<03:43, 20.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 410M/5.00G [00:51<03:10, 24.1MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  84%|‚ñà‚ñà‚ñà‚ñé| 3.23G/3.83G [00:51<00:14, 40.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 414M/5.00G [00:51<02:52, 26.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 420M/5.00G [00:51<02:28, 30.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  72%|‚ñà‚ñà‚ñâ | 3.56G/4.95G [00:51<00:32, 43.4MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   8%|‚ñç    | 424M/5.00G [00:51<02:15, 33.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 434M/5.00G [00:51<01:45, 43.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 441M/5.00G [00:51<01:32, 49.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 448M/5.00G [00:51<01:35, 47.8MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  86%|‚ñà‚ñà‚ñà‚ñç| 3.29G/3.83G [00:51<00:11, 48.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  73%|‚ñà‚ñà‚ñâ | 3.62G/4.95G [00:52<00:25, 51.7MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 453M/5.00G [00:52<02:24, 31.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 458M/5.00G [00:52<02:13, 34.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  75%|‚ñà‚ñà‚ñâ | 3.69G/4.95G [00:52<00:17, 70.2MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  88%|‚ñà‚ñà‚ñà‚ñå| 3.36G/3.83G [00:52<00:08, 57.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  76%|‚ñà‚ñà‚ñà | 3.74G/4.95G [00:52<00:15, 77.7MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  77%|‚ñà‚ñà‚ñà | 3.81G/4.95G [00:53<00:12, 89.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   9%|‚ñç    | 464M/5.00G [00:53<05:19, 14.2MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  89%|‚ñà‚ñà‚ñà‚ñå| 3.43G/3.83G [00:53<00:06, 65.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  78%|‚ñà‚ñà‚ñà‚ñè| 3.88G/4.95G [00:53<00:11, 93.7MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  91%|‚ñà‚ñà‚ñà‚ñã| 3.50G/3.83G [00:54<00:04, 70.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  10%|‚ñç    | 478M/5.00G [00:54<04:50, 15.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  11%|‚ñå    | 548M/5.00G [00:54<01:10, 63.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  80%|‚ñà‚ñà‚ñà‚ñâ | 3.94G/4.95G [00:54<00:09, 104MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  93%|‚ñà‚ñà‚ñà‚ñã| 3.56G/3.83G [00:55<00:03, 68.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  12%|‚ñå    | 617M/5.00G [00:55<01:19, 55.0MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  95%|‚ñà‚ñà‚ñà‚ñä| 3.63G/3.83G [00:57<00:03, 51.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  14%|‚ñã    | 685M/5.00G [00:57<01:44, 41.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  81%|‚ñà‚ñà‚ñà‚ñè| 4.01G/4.95G [00:57<00:21, 44.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  14%|‚ñã    | 694M/5.00G [00:58<02:01, 35.6MB/s]\u001b[A\nmodel-00003-of-00003.safetensors:  96%|‚ñà‚ñà‚ñà‚ñä| 3.70G/3.83G [00:59<00:03, 40.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  82%|‚ñà‚ñà‚ñà‚ñé| 4.08G/4.95G [01:00<00:25, 34.1MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors:  98%|‚ñà‚ñà‚ñà‚ñâ| 3.76G/3.83G [01:01<00:01, 36.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  15%|‚ñä    | 752M/5.00G [01:02<03:13, 22.0MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  84%|‚ñà‚ñà‚ñà‚ñé| 4.14G/4.95G [01:02<00:23, 34.3MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  16%|‚ñä    | 814M/5.00G [01:03<02:07, 32.8MB/s]\u001b[A\nmodel-00003-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà| 3.83G/3.83G [01:03<00:00, 60.7MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:  17%|‚ñä    | 841M/5.00G [01:03<01:48, 38.2MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  85%|‚ñà‚ñà‚ñà‚ñç| 4.21G/4.95G [01:03<00:17, 42.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  86%|‚ñà‚ñà‚ñà‚ñç| 4.28G/4.95G [01:04<00:12, 52.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  88%|‚ñà‚ñà‚ñà‚ñå| 4.35G/4.95G [01:04<00:08, 71.1MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  89%|‚ñà‚ñà‚ñà‚ñå| 4.41G/4.95G [01:04<00:05, 94.9MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  91%|‚ñà‚ñà‚ñà‚ñà‚ñå| 4.48G/4.95G [01:04<00:04, 112MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  92%|‚ñà‚ñà‚ñà‚ñà‚ñå| 4.55G/4.95G [01:05<00:03, 112MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  18%|‚ñâ    | 908M/5.00G [01:05<02:00, 33.8MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñã| 4.61G/4.95G [01:05<00:03, 109MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñã| 4.68G/4.95G [01:06<00:02, 107MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  20%|‚ñâ    | 975M/5.00G [01:06<01:42, 39.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  96%|‚ñà‚ñà‚ñà‚ñä| 4.75G/4.95G [01:07<00:02, 86.5MB/s]\u001b[A\u001b[A\n\nmodel-00002-of-00003.safetensors:  97%|‚ñà‚ñà‚ñà‚ñâ| 4.82G/4.95G [01:12<00:03, 35.5MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  21%|‚ñä   | 1.04G/5.00G [01:15<04:07, 16.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  22%|‚ñâ   | 1.11G/5.00G [01:16<02:55, 22.1MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors:  99%|‚ñà‚ñà‚ñà‚ñâ| 4.88G/4.95G [01:16<00:02, 24.8MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:  24%|‚ñâ   | 1.18G/5.00G [01:17<02:09, 29.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  25%|‚ñâ   | 1.24G/5.00G [01:17<01:42, 36.5MB/s]\u001b[A\n\nmodel-00002-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà| 4.95G/4.95G [01:20<00:00, 61.8MB/s]\u001b[A\u001b[A\n\nmodel-00001-of-00003.safetensors:  26%|‚ñà   | 1.31G/5.00G [01:20<01:48, 34.0MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  28%|‚ñà   | 1.38G/5.00G [01:21<01:32, 39.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  29%|‚ñà‚ñè  | 1.44G/5.00G [01:23<01:37, 36.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  30%|‚ñà‚ñè  | 1.51G/5.00G [01:24<01:20, 43.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  32%|‚ñà‚ñé  | 1.58G/5.00G [01:25<01:06, 51.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  34%|‚ñà‚ñé  | 1.71G/5.00G [01:25<00:36, 89.1MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  37%|‚ñà‚ñä   | 1.85G/5.00G [01:25<00:22, 141MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  38%|‚ñà‚ñâ   | 1.91G/5.00G [01:25<00:19, 162MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  42%|‚ñà‚ñà   | 2.11G/5.00G [01:25<00:10, 267MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  46%|‚ñà‚ñà‚ñé  | 2.32G/5.00G [01:26<00:07, 367MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  50%|‚ñà‚ñà‚ñå  | 2.52G/5.00G [01:26<00:04, 508MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  53%|‚ñà‚ñà‚ñã  | 2.65G/5.00G [01:26<00:06, 377MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  56%|‚ñà‚ñà‚ñä  | 2.79G/5.00G [01:26<00:04, 467MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  58%|‚ñà‚ñà‚ñâ  | 2.92G/5.00G [01:27<00:03, 568MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  61%|‚ñà‚ñà‚ñà  | 3.05G/5.00G [01:27<00:04, 450MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  64%|‚ñà‚ñà‚ñà‚ñè | 3.19G/5.00G [01:28<00:05, 326MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  65%|‚ñà‚ñà‚ñà‚ñé | 3.25G/5.00G [01:28<00:05, 300MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  66%|‚ñà‚ñà‚ñà‚ñé | 3.32G/5.00G [01:28<00:05, 283MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  69%|‚ñà‚ñà‚ñà‚ñç | 3.46G/5.00G [01:29<00:04, 314MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  72%|‚ñà‚ñà‚ñà‚ñå | 3.59G/5.00G [01:29<00:04, 333MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  73%|‚ñà‚ñà‚ñà‚ñã | 3.66G/5.00G [01:29<00:04, 303MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  76%|‚ñà‚ñà‚ñà‚ñä | 3.79G/5.00G [01:30<00:03, 311MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  77%|‚ñà‚ñà‚ñà‚ñä | 3.86G/5.00G [01:30<00:04, 264MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  80%|‚ñà‚ñà‚ñà‚ñâ | 3.99G/5.00G [01:31<00:04, 234MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  81%|‚ñà‚ñà‚ñà‚ñè| 4.06G/5.00G [01:33<00:09, 95.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  83%|‚ñà‚ñà‚ñà‚ñé| 4.13G/5.00G [01:34<00:10, 85.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  84%|‚ñà‚ñà‚ñà‚ñé| 4.19G/5.00G [01:35<00:09, 80.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  87%|‚ñà‚ñà‚ñà‚ñç| 4.33G/5.00G [01:36<00:07, 95.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  88%|‚ñà‚ñà‚ñà‚ñå| 4.40G/5.00G [01:38<00:08, 72.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  89%|‚ñà‚ñà‚ñà‚ñå| 4.46G/5.00G [01:38<00:06, 80.6MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  91%|‚ñà‚ñà‚ñà‚ñå| 4.53G/5.00G [01:39<00:04, 98.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  93%|‚ñà‚ñà‚ñà‚ñà‚ñã| 4.66G/5.00G [01:39<00:02, 151MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  95%|‚ñà‚ñà‚ñà‚ñà‚ñã| 4.73G/5.00G [01:39<00:01, 168MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  97%|‚ñà‚ñà‚ñà‚ñà‚ñä| 4.86G/5.00G [01:39<00:00, 226MB/s]\u001b[A\nmodel-00001-of-00003.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà| 5.00G/5.00G [01:40<00:00, 49.9MB/s]\u001b[A\nFetching 3 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:40<00:00, 33.54s/it]\nFetching 3 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:40<00:00, 33.54s/it]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:03<00:00, 21.14s/it]\ngeneration_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [00:00<00:00, 976kB/s]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [01:03<00:00, 21.20s/it]\ntrainable params: 79,953,920 || all params: 6,968,049,664 || trainable%: 1.1474\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\ntrainable params: 79,953,920 || all params: 6,968,049,664 || trainable%: 1.1474\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]\nLoading `train_dataloader` to estimate number of stepping batches.\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mType                \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m‚îÉ\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m‚îÇ model ‚îÇ PeftModelForCausalLM ‚îÇ  3.7 B ‚îÇ train ‚îÇ     0 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\u001b[1mTrainable params\u001b[0m: 80.0 M                                                        \n\u001b[1mNon-trainable params\u001b[0m: 3.7 B                                                     \n\u001b[1mTotal params\u001b[0m: 3.7 B                                                             \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 14.9 K                                  \n\u001b[1mModules in train mode\u001b[0m: 2242                                                     \n\u001b[1mModules in eval mode\u001b[0m: 423                                                       \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                  \n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 423 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n/usr/local/lib/python3.12/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: It is recommended to use `self.log('Train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n`Trainer.fit` stopped: `max_epochs=1` reached.\n\nStarting test set evaluation (Accuracy & ROC-AUC)...\n[TEST] Number of samples: 4112\nEvaluating on 4112 samples using Logits...\nSample 0: True=0, Prob(Yes)=0.0210\nSample 100: True=0, Prob(Yes)=0.0169\nSample 200: True=0, Prob(Yes)=0.0198\nSample 300: True=0, Prob(Yes)=0.0170\nSample 400: True=0, Prob(Yes)=0.0201\nSample 500: True=0, Prob(Yes)=0.0169\nSample 600: True=0, Prob(Yes)=0.0175\nSample 700: True=0, Prob(Yes)=0.0169\nSample 800: True=0, Prob(Yes)=0.0175\nSample 900: True=0, Prob(Yes)=0.0203\nSample 1000: True=0, Prob(Yes)=0.0208\nSample 1100: True=0, Prob(Yes)=0.0170\nSample 1200: True=0, Prob(Yes)=0.0177\nSample 1300: True=0, Prob(Yes)=0.0182\nSample 1400: True=0, Prob(Yes)=0.0202\nSample 1500: True=0, Prob(Yes)=0.0175\nSample 1600: True=0, Prob(Yes)=0.0238\nSample 1700: True=0, Prob(Yes)=0.0242\nSample 1800: True=0, Prob(Yes)=0.0167\nSample 1900: True=0, Prob(Yes)=0.0189\nSample 2000: True=0, Prob(Yes)=0.0166\nSample 2100: True=0, Prob(Yes)=0.0167\nSample 2200: True=0, Prob(Yes)=0.0168\nSample 2300: True=0, Prob(Yes)=0.0196\nSample 2400: True=0, Prob(Yes)=0.0170\nSample 2500: True=0, Prob(Yes)=0.0208\nSample 2600: True=0, Prob(Yes)=0.0191\nSample 2700: True=0, Prob(Yes)=0.0176\nSample 2800: True=0, Prob(Yes)=0.0168\nSample 2900: True=0, Prob(Yes)=0.0169\nSample 3000: True=0, Prob(Yes)=0.0170\nSample 3100: True=0, Prob(Yes)=0.0167\nSample 3200: True=0, Prob(Yes)=0.0173\nSample 3300: True=0, Prob(Yes)=0.0169\nSample 3400: True=0, Prob(Yes)=0.0167\nSample 3500: True=0, Prob(Yes)=0.0170\nSample 3600: True=0, Prob(Yes)=0.0170\nSample 3700: True=0, Prob(Yes)=0.0175\nSample 3800: True=0, Prob(Yes)=0.0218\nSample 3900: True=0, Prob(Yes)=0.0170\nSample 4000: True=0, Prob(Yes)=0.0172\nSample 4100: True=0, Prob(Yes)=0.0169\n\n=== Test Set Metrics ===\nAccuracy: 0.9686\nROC-AUC:  0.7262\n","output_type":"stream"}],"execution_count":10}]}