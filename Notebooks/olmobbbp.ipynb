{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n\n    # Optional: print the name of each GPU\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"CUDA is not available. No GPUs found.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install bitsandbytes\n! pip install peft\n! pip install --pre deepchem\n! pip install ai2-olmo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport torch\nimport pytorch_lightning as pl\nimport deepchem as dc\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom deepchem.molnet import load_bbbp\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom torch.nn.functional import softmax\n\nclass OlmoDataset(Dataset):\n    def __init__(self, mode=\"Train\", max_length=300):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n\n        tasks, datasets, transformers = load_bbbp(featurizer=\"raw\", splitter='scaffold')\n        train, valid, test = datasets\n        \n        self.task_names = tasks \n\n        self.mode = mode.lower()\n        if self.mode == \"train\":\n            self.data = train\n        elif self.mode == \"test\":\n            self.data = test\n        else:\n            self.data = train \n\n        self.max_length = max_length\n        self.samples = []\n        self._filldataset()\n\n    def _filldataset(self):\n        for i in range(len(self.data)):\n            smiles = self.data.ids[i]\n            labels = self.data.y[i]   \n            weights = self.data.w[i]\n\n            for task_idx, label in enumerate(labels):\n                if weights[task_idx] > 0:\n                    task_name = \"blood-brain barrier penetration\"\n                    self.samples.append(self._create_prompt(smiles, task_name, label))\n                     \n        print(f\"[{self.mode.upper()}] Number of samples: {len(self.samples)}\")\n\n    def _create_prompt(self, smiles, task_name, label):\n        eos_token = self.tokenizer.eos_token\n        # Classification: 1 = Yes (Penetrates), 0 = No\n        answer = \"Yes\" if label == 1.0 else \"No\"\n        \n        full_prompt = (\n            \"### Instruction:\\n\"\n            f\"Is the following molecule capable of {task_name}?\\n\"\n            f\"{smiles}\\n\\n\"\n            \"### Response:\\n\"\n            f\"{answer}{eos_token}\"\n        )\n        return full_prompt\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        text = self.samples[idx]\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        input_ids = encodings[\"input_ids\"].squeeze(0)\n        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n        labels = input_ids.clone()\n\n        separator = \"### Response:\\n\"\n        parts = text.split(separator)\n\n        if len(parts) >= 2:\n            prompt_text = parts[0] + separator\n            prompt_encodings = self.tokenizer(\n                prompt_text,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            prompt_len = prompt_encodings[\"input_ids\"].shape[1]\n\n            if prompt_len < len(labels):\n                labels[:prompt_len] = -100\n        \n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\nclass OLMO_QLoRA(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.peft_config = LoraConfig(\n            r=32,\n            lora_alpha=64,\n            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM \n        )\n\n    def configure_model(self):\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"Codemaster67/OLMo-7B-USPTO-1k-ZINC\",\n            quantization_config=self.bnb_config,\n            trust_remote_code=True,\n        )\n        self.model = prepare_model_for_kbit_training(self.model)\n        self.model = get_peft_model(self.model, self.peft_config)\n        self.model.print_trainable_parameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n    \n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        self.log(\"Train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n        return loss\n\n    def on_train_end(self):\n        if self.trainer.is_global_zero:\n            print(\"\\nStarting test set evaluation (Accuracy & ROC-AUC)...\")\n            \n            test_dataset = OlmoDataset(mode=\"test\", max_length=300)\n            test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n            \n            # Prepare special tokens for classification check\n            yes_token_id = self.tokenizer.encode(\"Yes\", add_special_tokens=False)[0]\n            no_token_id = self.tokenizer.encode(\"No\", add_special_tokens=False)[0]\n            \n            self.model.eval()\n            \n            y_true = []\n            y_probs = [] \n            y_pred = []  \n\n            print(f\"Evaluating on {len(test_loader)} samples using Logits...\")\n\n            with torch.no_grad():\n                for i, batch in enumerate(test_loader):\n                    batch = {k: v.to(self.device) for k, v in batch.items()}\n                    input_ids = batch[\"input_ids\"]\n                    labels = batch[\"labels\"]\n                    attention_mask = batch[\"attention_mask\"]\n\n                    # Find where the prompt ends\n                    response_mask = (labels != -100)\n                    if response_mask.sum() == 0: continue\n                    \n                    answer_start_index = response_mask.int().argmax(dim=1).item()\n                    \n                    if answer_start_index > 0:\n                        prompt_ids = input_ids[:, :answer_start_index]\n                        prompt_mask = attention_mask[:, :answer_start_index]\n                    else:\n                        continue \n\n                    outputs = self.model(input_ids=prompt_ids, attention_mask=prompt_mask)\n                    # Get logits of the last token generated\n                    next_token_logits = outputs.logits[:, -1, :]\n                    \n                    # Extract logits for \"Yes\" and \"No\"\n                    yes_logit = next_token_logits[:, yes_token_id]\n                    no_logit = next_token_logits[:, no_token_id]\n                    \n                    relevant_logits = torch.stack([no_logit, yes_logit], dim=1)\n                    probs = softmax(relevant_logits, dim=1)\n                    \n                    # Probability of class 1 (\"Yes\")\n                    prob_yes = probs[:, 1].item()\n                    prediction = 1 if prob_yes > 0.5 else 0\n                    \n                    # Get True Label\n                    truth_id = labels[0][answer_start_index].item()\n                    if truth_id == yes_token_id:\n                        true_label = 1\n                    elif truth_id == no_token_id:\n                        true_label = 0\n                    else:\n                        continue \n\n                    y_true.append(true_label)\n                    y_probs.append(prob_yes)\n                    y_pred.append(prediction)\n\n                    if i % 100 == 0:\n                        print(f\"Sample {i}: True={true_label}, Prob(Yes)={prob_yes:.4f}\")\n\n            acc = accuracy_score(y_true, y_pred)\n\n            roc = roc_auc_score(y_true, y_probs)\n\n\n            print(\"\\n=== Test Set Metrics ===\")\n            print(f\"Accuracy: {acc:.4f}\")\n            print(f\"ROC-AUC:  {roc:.4f}\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n        \n        total_steps = self.trainer.estimated_stepping_batches\n        \n        warmup_steps = int(0.15*total_steps)\n        scheduler_warmup = LinearLR(optimizer, start_factor=0.001, end_factor=1.0, total_iters=warmup_steps)\n        scheduler_cosine = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps)\n        scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[warmup_steps])\n        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"}}\n\nif __name__ == \"__main__\":\n    dataset = OlmoDataset(mode=\"train\")\n\n    train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n\n    trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            devices=2,\n            strategy=\"ddp\",\n            max_epochs=10,\n            precision=\"16-mixed\",\n            accumulate_grad_batches=4, \n            gradient_clip_val=0.5,\n            log_every_n_steps=10,\n            enable_checkpointing = False,\n        )\n    \n    model = OLMO_QLoRA()\n    \n    # Only passing train_loader\n    trainer.fit(model, train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}