{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\n# Check if CUDA is available\nif torch.cuda.is_available():\n    # Get the number of available GPUs\n    num_gpus = torch.cuda.device_count()\n    print(f\"Number of available GPUs: {num_gpus}\")\n\n    # Optional: print the name of each GPU\n    for i in range(num_gpus):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\nelse:\n    print(\"CUDA is not available. No GPUs found.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install bitsandbytes\n! pip install peft\n! pip install --pre deepchem\n! pip install ai2-olmo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train.py\nimport torch\nimport pytorch_lightning as pl\nimport deepchem as dc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nfrom deepchem.molnet import load_lipo \nfrom torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\nfrom sklearn.metrics import mean_squared_error\nimport seaborn as sns\nimport re\n\nclass OlmoDataset(Dataset):\n    def __init__(self, mode=\"Train\", max_length=350):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        # Load Lipophilicity dataset\n        tasks, datasets, transformers = load_lipo(featurizer=\"raw\", splitter='scaffold')\n        train, valid, test = datasets\n\n        self.mode = mode.lower()\n        if self.mode == \"train\":\n            self.data = train\n        elif self.mode == \"test\":\n            self.data = test\n        \n        self.max_length = max_length\n        self.samples = []\n        self._filldataset()\n\n    def _filldataset(self):\n        for i in range(len(self.data)):\n            smiles = self.data.ids[i]\n            labels = self.data.y[i]   \n            weights = self.data.w[i] \n\n            for task_idx, label in enumerate(labels):\n                if weights[task_idx] > 0:\n                    self.samples.append(self._create_prompt(smiles, label))\n            \n        print(f\"[{self.mode.upper()}] Number of samples: {len(self.samples)}\")\n\n    def _create_prompt(self, smiles, label):\n        eos_token = self.tokenizer.eos_token\n        answer = f\"{label:.5f}\"\n\n        full_prompt = (\n            \"### Instruction:\\n\"\n            \"Predict the Lipophilicity (LogD) \"\n            f\"for the following molecule:\\n{smiles}\\n\\n\"\n            \"### Response:\\n\"\n            f\"{answer}{eos_token}\"\n        )\n        return full_prompt\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        text = self.samples[idx]\n        encodings = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n        input_ids = encodings[\"input_ids\"].squeeze(0)\n        attention_mask = encodings[\"attention_mask\"].squeeze(0)\n        labels = input_ids.clone()\n\n        separator = \"### Response:\\n\"\n        parts = text.split(separator)\n\n        if len(parts) >= 2:\n            prompt_text = parts[0] + separator\n            prompt_encodings = self.tokenizer(\n                prompt_text,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors=\"pt\"\n            )\n            prompt_len = prompt_encodings[\"input_ids\"].shape[1]\n\n            if prompt_len < len(labels):\n                labels[:prompt_len] = -100\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\nclass OLMO_QLoRA(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            trust_remote_code=True,\n            padding_side=\"right\"\n        )\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16\n        )\n\n        self.peft_config = LoraConfig(\n            r=32,\n            lora_alpha=64,\n            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM \n        )\n        \n    def configure_model(self):\n        self.model = AutoModelForCausalLM.from_pretrained(\n            \"allenai/OLMo-7B-hf\",\n            quantization_config=self.bnb_config,\n            trust_remote_code=True,\n        )\n        self.model = prepare_model_for_kbit_training(self.model)\n        self.model = get_peft_model(self.model, self.peft_config)\n        self.model.print_trainable_parameters()\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        return self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        \n    def training_step(self, batch, batch_idx):\n        outputs = self(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n            labels=batch[\"labels\"]\n        )\n        loss = outputs.loss\n        self.log(\"Train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, logger=True)\n        return loss\n\n    def on_train_end(self):\n            if self.trainer.is_global_zero:\n                print(\"\\nStarting test set evaluation (RMSE) after training...\")\n\n                test_dataset = OlmoDataset(mode=\"test\", max_length=350)\n                test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n                self.model.eval()\n\n                preds = []\n                trues = []\n                \n\n                true_values = []\n                for i in range(len(test_dataset.data)):\n                     if test_dataset.data.w[i][0] > 0:\n                          true_values.append(test_dataset.data.y[i][0])\n                \n                print(f\"Evaluating on {len(test_loader)} samples...\")\n\n                with torch.no_grad():\n                    for i, batch in enumerate(test_loader):\n                        batch = {k: v.to(self.device) for k, v in batch.items()}\n\n                        input_ids = batch[\"input_ids\"]\n                        labels = batch[\"labels\"]\n                        attention_mask = batch[\"attention_mask\"]\n\n                        response_mask = (labels != -100)\n                        if response_mask.sum() == 0:\n                             continue\n                             \n                        answer_start_index = response_mask.int().argmax(dim=1).item()\n\n                        if answer_start_index > 0:\n                            prompt_ids = input_ids[:, :answer_start_index]\n                            prompt_mask = attention_mask[:, :answer_start_index]\n                        else:\n                            prompt_ids = input_ids\n                            prompt_mask = attention_mask\n\n                        outputs = self.model.generate(\n                            input_ids=prompt_ids,\n                            attention_mask=prompt_mask,\n                            max_new_tokens=10,\n                            pad_token_id=self.tokenizer.eos_token_id,\n                            do_sample=False\n                        )\n\n                        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n                        \n                        try:\n                            if \"### Response:\" in generated_text:\n                                response_part = generated_text.split(\"### Response:\")[-1].strip()\n                            else:\n                                response_part = generated_text.strip()\n\n                            match = re.search(r\"(?<!\\w)-?\\d+(?:\\.\\d+)?(?!\\w)\", response_part)\n                            if match:\n                                val = float(match.group())\n                            else:\n                                val = 0.0\n                        except Exception:\n                            val = 0.0\n\n                        preds.append(val)\n                        trues.append(true_values[i])\n\n                        if i % 50 == 0:\n                            print(f\"Sample {i}: True={true_values[i]:.5f}, Pred={val:.5f}\")\n\n                preds = np.array(preds)\n                trues = np.array(trues)\n\n                rmse = np.sqrt(mean_squared_error(trues, preds))\n\n                print(\"\\n=== Test Set Metrics ===\")\n                print(f\"RMSE: {rmse:.4f}\")\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1e-4)\n\n        total_steps = self.trainer.estimated_stepping_batches\n\n        warmup_steps = int(0.15*total_steps)\n        scheduler_warmup = LinearLR(\n            optimizer,\n            start_factor=0.01,\n            end_factor=1.0,\n            total_iters=warmup_steps,\n        )\n\n        scheduler_cosine = CosineAnnealingLR(\n            optimizer,\n            T_max=total_steps - warmup_steps,\n        )\n\n        scheduler = SequentialLR(\n            optimizer,\n            schedulers=[scheduler_warmup, scheduler_cosine],\n            milestones=[warmup_steps]\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"step\"\n            }\n        }\n\nif __name__ == \"__main__\":\n    dataset = OlmoDataset(mode=\"train\")\n\n    train_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n\n    trainer = pl.Trainer(\n            accelerator=\"gpu\",\n            devices=2,\n            strategy=\"ddp\",\n            max_epochs=5,          \n            precision=\"16-mixed\",\n            accumulate_grad_batches=8,\n            enable_checkpointing=False,\n            gradient_clip_val=1,\n        )\n\n    model = OLMO_QLoRA()\n\n    trainer.fit(model, train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}